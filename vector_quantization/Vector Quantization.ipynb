{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchtools.vq import VectorQuantize # https://github.com/pabloppp/pytorch-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[0, 0],[0, 1],[1, 0],[1, 1]]).float()\n",
    "y = torch.tensor([[0],[1],[1],[0]]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=8, out_features=1, bias=True)\n",
       "  (1): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = nn.Sequential(\n",
    "    nn.Linear(2, 8),\n",
    "    nn.Tanh()\n",
    ")\n",
    "\n",
    "l2 = nn.Sequential(\n",
    "    nn.Linear(8, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "def quantize(x):\n",
    "    q = (x > 0.5).float()\n",
    "    q.requires_grad = True\n",
    "    return q\n",
    "\n",
    "vq = VectorQuantize(8, 8)\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        try:\n",
    "            nn.init.xavier_uniform_(m.weight.data)\n",
    "            m.bias.data.fill_(0)\n",
    "        except AttributeError:\n",
    "            print(\"Skipping initialization of \", classname)\n",
    "        \n",
    "l1.apply(weights_init)\n",
    "l2.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 0.7257276773452759 [[0.40045949816703796], [0.5635364651679993], [0.5805628895759583], [0.44601789116859436]]\n",
      "600 0.2603875994682312 [[0.23179921507835388], [0.7389718294143677], [0.741919755935669], [0.26817086338996887]]\n",
      "900 0.07400226593017578 [[0.12879358232021332], [0.8632490634918213], [0.8614370226860046], [0.13969239592552185]]\n",
      "1200 0.030479181557893753 [[0.08482348173856735], [0.9130933880805969], [0.9108487963676453], [0.08822393417358398]]\n",
      "1500 0.016016945242881775 [[0.06253042817115784], [0.9373887777328491], [0.9352784752845764], [0.0632285475730896]]\n",
      "1800 0.009681724943220615 [[0.04920986294746399], [0.9515368342399597], [0.9496293663978577], [0.048726122826337814]]\n",
      "2100 0.006383006926625967 [[0.04033684730529785], [0.9607846736907959], [0.9590693116188049], [0.039278268814086914]]\n",
      "2400 0.004459144547581673 [[0.03397722542285919], [0.9673147797584534], [0.9657679200172424], [0.03262719884514809]]\n",
      "2700 0.0032450323924422264 [[0.029176900163292885], [0.9721834063529968], [0.9707825779914856], [0.027682490646839142]]\n",
      "3000 0.002433530520647764 [[0.025412648916244507], [0.9759609699249268], [0.9746870398521423], [0.023855961859226227]]\n"
     ]
    }
   ],
   "source": [
    "# Regular MLP\n",
    "optimizer = optim.Adam(list(l1.parameters())+list(l2.parameters()), lr=0.001)\n",
    "\n",
    "for i in range(3000):\n",
    "    pred = l2(l1(X))\n",
    "    loss = (pred - y).pow(2).sum()\n",
    "    \n",
    "    # first backpropagate l2\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (i+1) % 300 == 0:\n",
    "        print(i+1, loss.item(), pred.detach().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 0.8163377046585083 [[0.43204426765441895], [0.4940449297428131], [0.6399938464164734], [0.4940449297428131]]\n",
      "600 0.7620685696601868 [[0.37148717045783997], [0.5079894065856934], [0.6479514241218567], [0.5079894065856934]]\n",
      "900 0.5148895382881165 [[0.19342926144599915], [0.6210345029830933], [0.70762038230896], [0.49837133288383484]]\n",
      "1200 0.4038856029510498 [[0.17204031348228455], [0.679912269115448], [0.7246944904327393], [0.4427622854709625]]\n",
      "1500 0.24194872379302979 [[0.11756282299757004], [0.8646786212921143], [0.7200552821159363], [0.3625558912754059]]\n",
      "1800 0.18765972554683685 [[0.08906986564397812], [0.8989024758338928], [0.7289723753929138], [0.30991870164871216]]\n",
      "2100 0.1528913378715515 [[0.06801759451627731], [0.9119315147399902], [0.7439187169075012], [0.27373576164245605]]\n",
      "2400 0.125566765666008 [[0.0544358491897583], [0.9334274530410767], [0.7608671188354492], [0.24695558845996857]]\n",
      "2700 0.10639818012714386 [[0.044791653752326965], [0.9356807470321655], [0.7776594161987305], [0.22543200850486755]]\n",
      "3000 0.09061049669981003 [[0.04209055379033089], [0.9422435760498047], [0.7933925986289978], [0.20692138373851776]]\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(list(l1.parameters())+list(l2.parameters()), lr=0.001)\n",
    "\n",
    "for i in range(3000):\n",
    "    discrete = l1(X)\n",
    "    quantized = quantize(discrete)\n",
    "    pred = l2(quantized)\n",
    "    loss = (pred - y).pow(2).sum()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    discrete.backward(quantized.grad)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if (i+1) % 300 == 0:\n",
    "        print(i+1, loss.item(), pred.detach().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 1.1997461318969727 [[0.49139320850372314], [0.5096949934959412], [0.5008904933929443], [0.49806979298591614]]\n",
      "600 0.9896625280380249 [[0.4808160960674286], [0.5077378153800964], [0.5042719841003418], [0.5078343749046326]]\n",
      "900 0.9828028678894043 [[0.4890400767326355], [0.5129714608192444], [0.4973489046096802], [0.500495433807373]]\n",
      "1200 0.961938202381134 [[0.48719459772109985], [0.5423017740249634], [0.48038920760154724], [0.4864928424358368]]\n",
      "1500 0.6794148087501526 [[0.39802515506744385], [0.6427853107452393], [0.5492122769355774], [0.40654125809669495]]\n",
      "1800 0.37637051939964294 [[0.28365492820739746], [0.7145309448242188], [0.6965494155883789], [0.30399253964424133]]\n",
      "2100 0.1721193790435791 [[0.19136583805084229], [0.793925940990448], [0.8009101748466492], [0.21235743165016174]]\n",
      "2400 0.08564083278179169 [[0.13530048727989197], [0.8496260046958923], [0.859707236289978], [0.15446625649929047]]\n",
      "2700 0.04983270913362503 [[0.10270783305168152], [0.88374263048172], [0.8937110900878906], [0.11920846253633499]]\n",
      "3000 0.032186780124902725 [[0.08222480863332748], [0.9058467149734497], [0.9151749610900879], [0.09628817439079285]]\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(list(l1.parameters())+list(l2.parameters())+list(vq.parameters()), lr=0.001)\n",
    "# optimizer = optim.Adam(l2.parameters(), lr=0.001)\n",
    "\n",
    "for i in range(3000):\n",
    "    e = l1(X)\n",
    "    q, q_grad = vq(e)\n",
    "    pred = l2(q)\n",
    "    loss_recon = (pred - y).pow(2).sum()   \n",
    "    loss_vq = (q_grad - e.detach()).pow(2).sum()\n",
    "    loss_commit = (e - q_grad.detach()).pow(2).sum() * 0.25\n",
    "    \n",
    "    loss = loss_recon + loss_vq + loss_commit\n",
    "    \n",
    "    optimizer.zero_grad()    \n",
    "    loss.backward()    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if (i+1) % 300 == 0:\n",
    "        print(i+1, loss.item(), pred.detach().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08205469697713852], [0.9063223600387573], [0.9156545400619507], [0.09569525718688965]]\n",
      "[[0.9026834964752197], [0.9993128776550293], [0.9990249872207642], [0.9834163188934326]]\n",
      "[[0.08216940611600876], [0.9059074521064758], [0.9152330756187439], [0.09622524678707123]]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    pred = l2(l1(X))\n",
    "    pred_q = l2(quantize(l1(X)))\n",
    "    pred_q2 = l2(vq(l1(X))[0])\n",
    "    \n",
    "    print(pred.numpy().tolist())\n",
    "    print(pred_q.numpy().tolist())\n",
    "    print(pred_q2.numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
