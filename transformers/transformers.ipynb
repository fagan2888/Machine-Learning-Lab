{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformers.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "gzgk01oREc7W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import transformer\n",
        "import glob\n",
        "import pickle\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aWh1dL4BFbk3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6a80d551-6ca3-4f2e-c4e6-93941323324a"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "u4evY_BfEc7h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "aef9d991-dc6f-44ae-f859-b62dcad12849"
      },
      "cell_type": "code",
      "source": [
        "# load the dataset\n",
        "poems = []\n",
        "\n",
        "for path in glob.iglob(\"drive/My Drive/datasets/poemas_machado/*.txt\"):\n",
        "    with open(path, 'r') as f:\n",
        "        x = f.read()\n",
        "        poems.append(x.lower().split(' '))\n",
        "        \n",
        "print(poems[0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['a', 'don', 'miguel', 'de', 'unamuno', '\\n', 'por', 'su', 'libro', 'vida', 'de', 'don', 'quijote', 'y', 'sancho', '.', '\\n', 'este', 'donquijotesco', '\\n', 'don', 'miguel', 'de', 'unamuno', ',', 'fuerte', 'vasco', ',', '\\n', 'lleva', 'el', 'arnés', 'grotesco', '\\n', 'y', 'el', 'irrisorio', 'casco', '\\n', 'del', 'buen', 'manchego', '.', 'don', 'miguel', 'camina', ',', '\\n', 'jinete', 'de', 'quimérica', 'montura', ',', '\\n', 'metiendo', 'espuela', 'de', 'oro', 'a', 'su', 'locura', ',', '\\n', 'sin', 'miedo', 'de', 'la', 'lengua', 'que', 'malsina', '.', '\\n', 'a', 'un', 'pueblo', 'de', 'arrieros', ',', '\\n', 'lechuzos', 'y', 'tahúres', 'y', 'logreros', '\\n', 'dicta', 'lecciones', 'de', 'caballería', '.', '\\n', 'y', 'el', 'alma', 'desalmada', 'de', 'su', 'raza', ',', '\\n', 'que', 'bajo', 'el', 'golpe', 'de', 'su', 'férrea', 'maza', '\\n', 'aún', 'durme', ',', 'puede', 'que', 'despierte', 'un', 'día', '.', '\\n', 'quiere', 'enseñar', 'el', 'ceño', 'de', 'la', 'duda', ',', '\\n', 'antes', 'de', 'que', 'cabalgue', ',', 'el', 'caballero', ';', '\\n', 'cual', 'nuevo', 'hamlet', ',', 'a', 'mirar', 'desnuda', '\\n', 'cerca', 'del', 'corazón', 'la', 'hoja', 'de', 'acero', '.', '\\n', 'tiene', 'el', 'aliento', 'de', 'una', 'estirpe', 'fuerte', '\\n', 'que', 'soñó', 'más', 'allá', 'de', 'sus', 'hogares', ',', '\\n', 'y', 'que', 'el', 'oro', 'buscó', 'tras', 'de', 'los', 'mares', '.', '\\n', 'él', 'señala', 'la', 'gloria', 'tras', 'la', 'muerte', '.', '\\n', 'quiere', 'ser', 'fundador', ',', 'y', 'dice', ':', 'creo', ';', '\\n', 'dios', 'y', 'adelante', 'el', 'ánima', 'española', '.', '.', '.', '\\n', 'y', 'es', 'tan', 'bueno', 'y', 'mejor', 'que', 'fue', 'loyola', ':', '\\n', 'sabe', 'a', 'jesús', 'y', 'escupe', 'al', 'fariseo', '.', '\\n', '<end>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DCL7WL4gEc7r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "83c3152b-ffbc-43d4-94c5-975bf2190223"
      },
      "cell_type": "code",
      "source": [
        "with open('word_idx.pkl', 'rb') as file:\n",
        "    word_idx = pickle.load(file)\n",
        "    \n",
        "with open('idx_word.pkl', 'rb') as file:\n",
        "    idx_word = pickle.load(file)\n",
        "\n",
        "vocabulary_size = len(word_idx) + 1\n",
        "print(vocabulary_size)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6299\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0VR-nwheEc76",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "287771a9-edd3-42da-b9b9-3336e9e3f2eb"
      },
      "cell_type": "code",
      "source": [
        "tokenized = []\n",
        "for poem in poems:\n",
        "    tokenized.append([word_idx[word] for word in poem])\n",
        "\n",
        "print(poems[0])\n",
        "print(tokenized[0])\n",
        "print(\"---\")\n",
        "print(poems[0][7], \"->\", word_idx[poems[0][7]])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['a', 'don', 'miguel', 'de', 'unamuno', '\\n', 'por', 'su', 'libro', 'vida', 'de', 'don', 'quijote', 'y', 'sancho', '.', '\\n', 'este', 'donquijotesco', '\\n', 'don', 'miguel', 'de', 'unamuno', ',', 'fuerte', 'vasco', ',', '\\n', 'lleva', 'el', 'arnés', 'grotesco', '\\n', 'y', 'el', 'irrisorio', 'casco', '\\n', 'del', 'buen', 'manchego', '.', 'don', 'miguel', 'camina', ',', '\\n', 'jinete', 'de', 'quimérica', 'montura', ',', '\\n', 'metiendo', 'espuela', 'de', 'oro', 'a', 'su', 'locura', ',', '\\n', 'sin', 'miedo', 'de', 'la', 'lengua', 'que', 'malsina', '.', '\\n', 'a', 'un', 'pueblo', 'de', 'arrieros', ',', '\\n', 'lechuzos', 'y', 'tahúres', 'y', 'logreros', '\\n', 'dicta', 'lecciones', 'de', 'caballería', '.', '\\n', 'y', 'el', 'alma', 'desalmada', 'de', 'su', 'raza', ',', '\\n', 'que', 'bajo', 'el', 'golpe', 'de', 'su', 'férrea', 'maza', '\\n', 'aún', 'durme', ',', 'puede', 'que', 'despierte', 'un', 'día', '.', '\\n', 'quiere', 'enseñar', 'el', 'ceño', 'de', 'la', 'duda', ',', '\\n', 'antes', 'de', 'que', 'cabalgue', ',', 'el', 'caballero', ';', '\\n', 'cual', 'nuevo', 'hamlet', ',', 'a', 'mirar', 'desnuda', '\\n', 'cerca', 'del', 'corazón', 'la', 'hoja', 'de', 'acero', '.', '\\n', 'tiene', 'el', 'aliento', 'de', 'una', 'estirpe', 'fuerte', '\\n', 'que', 'soñó', 'más', 'allá', 'de', 'sus', 'hogares', ',', '\\n', 'y', 'que', 'el', 'oro', 'buscó', 'tras', 'de', 'los', 'mares', '.', '\\n', 'él', 'señala', 'la', 'gloria', 'tras', 'la', 'muerte', '.', '\\n', 'quiere', 'ser', 'fundador', ',', 'y', 'dice', ':', 'creo', ';', '\\n', 'dios', 'y', 'adelante', 'el', 'ánima', 'española', '.', '.', '.', '\\n', 'y', 'es', 'tan', 'bueno', 'y', 'mejor', 'que', 'fue', 'loyola', ':', '\\n', 'sabe', 'a', 'jesús', 'y', 'escupe', 'al', 'fariseo', '.', '\\n', '<end>']\n",
            "[12, 187, 293, 4, 588, 1, 28, 18, 292, 91, 4, 187, 2096, 5, 3891, 3, 1, 136, 3892, 1, 187, 293, 4, 588, 2, 249, 3893, 2, 1, 177, 7, 1387, 1419, 1, 5, 7, 2611, 1603, 1, 13, 309, 825, 3, 187, 293, 416, 2, 1, 1745, 4, 2612, 3894, 2, 1, 3895, 2097, 4, 114, 12, 18, 1102, 2, 1, 58, 351, 4, 6, 1299, 9, 3896, 3, 1, 12, 10, 378, 4, 1308, 2, 1, 3897, 5, 3898, 5, 3899, 1, 3900, 3901, 4, 3902, 3, 1, 5, 7, 66, 3903, 4, 18, 1039, 2, 1, 9, 65, 7, 722, 4, 18, 2029, 1571, 1, 306, 3904, 2, 301, 9, 3905, 10, 60, 3, 1, 480, 3906, 7, 418, 4, 6, 3907, 2, 1, 270, 4, 9, 2098, 2, 7, 447, 16, 1, 156, 430, 2613, 2, 12, 283, 985, 1, 219, 13, 38, 6, 1056, 4, 1332, 3, 1, 80, 7, 457, 4, 22, 1604, 249, 1, 9, 2099, 53, 197, 4, 37, 1275, 2, 1, 5, 9, 7, 114, 2100, 160, 4, 11, 272, 3, 1, 192, 3908, 6, 545, 160, 6, 113, 3, 1, 480, 298, 3909, 2, 5, 122, 27, 3910, 16, 1, 48, 5, 1983, 7, 3911, 2101, 3, 3, 3, 1, 5, 23, 145, 930, 5, 353, 9, 148, 3912, 27, 1, 129, 12, 504, 5, 3913, 20, 3914, 3, 1, 24]\n",
            "---\n",
            "su -> 18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UAFXWGFiEc9i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "81c6e0f0-4b7d-4cbe-995b-ffc8064b3390"
      },
      "cell_type": "code",
      "source": [
        "lengths = [len(sequence) for sequence in tokenized]\n",
        "print(max(lengths))\n",
        "print(min(lengths))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4758\n",
            "13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sPmJcBJTEc9t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "11519988-3eac-47aa-8521-93bec53baf05"
      },
      "cell_type": "code",
      "source": [
        "max_seq_length = 600 #4758\n",
        "\n",
        "padded = []\n",
        "for sequence in tokenized:\n",
        "    trimmed = sequence[-max_seq_length:]\n",
        "    padding = [0] * (max_seq_length - len(trimmed))\n",
        "    padded.append(padding + trimmed)\n",
        "    \n",
        "padded = np.array(padded)\n",
        "print(padded.shape, padded[:,:-1].shape, padded[:,1:].shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(445, 600) (445, 599) (445, 599)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pFGyhuk2Ec-K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create data loader\n",
        "batch_size = 4\n",
        "\n",
        "dataset = TensorDataset(torch.from_numpy(padded[:,:-1]), torch.from_numpy(padded[:,1:]))\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q2wGYeA9Ec-S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4c9e2e40-1bb1-4ae6-8292-217dde8202ae"
      },
      "cell_type": "code",
      "source": [
        "x, y = next(iter(dataloader))\n",
        "\n",
        "print(x.shape, y.shape)\n",
        "\n",
        "print(x[0, -10:])\n",
        "print(y[0, -10:])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 599]) torch.Size([4, 599])\n",
            "tensor([  86, 5342,   14,   15, 5343, 5344,    4,  275,   14,    1])\n",
            "tensor([5342,   14,   15, 5343, 5344,    4,  275,   14,    1,   24])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tyG5fOx8Ec-c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = transformer.Transformer(vocabulary_size, 512, max_seq_length-1, blocks=12, heads=8)\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "# model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "E_jxtbQQEc-g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1547
        },
        "outputId": "c0fca392-1e43-4be4-8955-e90dbed12647"
      },
      "cell_type": "code",
      "source": [
        "# training\n",
        "epochs = 10\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, betas=(0.9, 0.98), eps=1e-9)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
        "\n",
        "model.cuda()\n",
        "model.train()\n",
        "for e in range(1, epochs+1):\n",
        "    scheduler.step()\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "    batch = 0\n",
        "    for x, y in dataloader:\n",
        "        x, y = x.cuda(), y.cuda()\n",
        "        batch += 1\n",
        "        optimizer.zero_grad()\n",
        "    \n",
        "        mask = transformer.gen_target_mask(x.cpu(), 0).cuda()\n",
        "    \n",
        "        preds = model.encoder(x, mask)\n",
        "        preds = model.out(preds)\n",
        "        \n",
        "        loss = F.cross_entropy(preds.view(-1, preds.size(-1)), y.view(-1)) #, ignore_index=0)\n",
        "        total_loss += loss.item()\n",
        "    \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        equals = torch.argmax(F.softmax(preds, dim=-1), dim=-1).view(-1) == y.view(-1)\n",
        "        total_accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "    \n",
        "        if batch % 14 == 0:\n",
        "            print(f\"EPOCH {e} ({batch}/{len(dataloader)}) - loss {total_loss/batch:.4f} - acc {total_accuracy/batch:.4f}\") \n",
        "\n",
        "    print(f\"EPOCH {e} - loss {total_loss/len(dataloader):.4f} - acc {total_accuracy/len(dataloader):.4f} ---------------------- \")"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH 1 (14/112) - loss 0.0215 - acc 0.9953\n",
            "EPOCH 1 (28/112) - loss 0.0207 - acc 0.9958\n",
            "EPOCH 1 (42/112) - loss 0.0205 - acc 0.9958\n",
            "EPOCH 1 (56/112) - loss 0.0205 - acc 0.9959\n",
            "EPOCH 1 (70/112) - loss 0.0204 - acc 0.9959\n",
            "EPOCH 1 (84/112) - loss 0.0202 - acc 0.9959\n",
            "EPOCH 1 (98/112) - loss 0.0201 - acc 0.9960\n",
            "EPOCH 1 (112/112) - loss 0.0194 - acc 0.9961\n",
            "EPOCH 1 - loss 0.0194 - acc 0.9961 ---------------------- \n",
            "EPOCH 2 (14/112) - loss 0.0140 - acc 0.9970\n",
            "EPOCH 2 (28/112) - loss 0.0151 - acc 0.9969\n",
            "EPOCH 2 (42/112) - loss 0.0156 - acc 0.9968\n",
            "EPOCH 2 (56/112) - loss 0.0154 - acc 0.9969\n",
            "EPOCH 2 (70/112) - loss 0.0156 - acc 0.9968\n",
            "EPOCH 2 (84/112) - loss 0.0153 - acc 0.9968\n",
            "EPOCH 2 (98/112) - loss 0.0154 - acc 0.9968\n",
            "EPOCH 2 (112/112) - loss 0.0157 - acc 0.9968\n",
            "EPOCH 2 - loss 0.0157 - acc 0.9968 ---------------------- \n",
            "EPOCH 3 (14/112) - loss 0.0145 - acc 0.9971\n",
            "EPOCH 3 (28/112) - loss 0.0135 - acc 0.9973\n",
            "EPOCH 3 (42/112) - loss 0.0138 - acc 0.9974\n",
            "EPOCH 3 (56/112) - loss 0.0140 - acc 0.9973\n",
            "EPOCH 3 (70/112) - loss 0.0142 - acc 0.9972\n",
            "EPOCH 3 (84/112) - loss 0.0142 - acc 0.9972\n",
            "EPOCH 3 (98/112) - loss 0.0142 - acc 0.9972\n",
            "EPOCH 3 (112/112) - loss 0.0141 - acc 0.9972\n",
            "EPOCH 3 - loss 0.0141 - acc 0.9972 ---------------------- \n",
            "EPOCH 4 (14/112) - loss 0.0127 - acc 0.9977\n",
            "EPOCH 4 (28/112) - loss 0.0129 - acc 0.9976\n",
            "EPOCH 4 (42/112) - loss 0.0135 - acc 0.9975\n",
            "EPOCH 4 (56/112) - loss 0.0132 - acc 0.9974\n",
            "EPOCH 4 (70/112) - loss 0.0132 - acc 0.9974\n",
            "EPOCH 4 (84/112) - loss 0.0131 - acc 0.9974\n",
            "EPOCH 4 (98/112) - loss 0.0133 - acc 0.9973\n",
            "EPOCH 4 (112/112) - loss 0.0134 - acc 0.9973\n",
            "EPOCH 4 - loss 0.0134 - acc 0.9973 ---------------------- \n",
            "EPOCH 5 (14/112) - loss 0.0133 - acc 0.9973\n",
            "EPOCH 5 (28/112) - loss 0.0131 - acc 0.9975\n",
            "EPOCH 5 (42/112) - loss 0.0128 - acc 0.9975\n",
            "EPOCH 5 (56/112) - loss 0.0126 - acc 0.9975\n",
            "EPOCH 5 (70/112) - loss 0.0129 - acc 0.9975\n",
            "EPOCH 5 (84/112) - loss 0.0128 - acc 0.9975\n",
            "EPOCH 5 (98/112) - loss 0.0127 - acc 0.9975\n",
            "EPOCH 5 (112/112) - loss 0.0126 - acc 0.9975\n",
            "EPOCH 5 - loss 0.0126 - acc 0.9975 ---------------------- \n",
            "EPOCH 6 (14/112) - loss 0.0123 - acc 0.9976\n",
            "EPOCH 6 (28/112) - loss 0.0122 - acc 0.9975\n",
            "EPOCH 6 (42/112) - loss 0.0121 - acc 0.9974\n",
            "EPOCH 6 (56/112) - loss 0.0120 - acc 0.9975\n",
            "EPOCH 6 (70/112) - loss 0.0120 - acc 0.9975\n",
            "EPOCH 6 (84/112) - loss 0.0121 - acc 0.9975\n",
            "EPOCH 6 (98/112) - loss 0.0122 - acc 0.9975\n",
            "EPOCH 6 (112/112) - loss 0.0123 - acc 0.9975\n",
            "EPOCH 6 - loss 0.0123 - acc 0.9975 ---------------------- \n",
            "EPOCH 7 (14/112) - loss 0.0116 - acc 0.9974\n",
            "EPOCH 7 (28/112) - loss 0.0121 - acc 0.9974\n",
            "EPOCH 7 (42/112) - loss 0.0118 - acc 0.9975\n",
            "EPOCH 7 (56/112) - loss 0.0118 - acc 0.9975\n",
            "EPOCH 7 (70/112) - loss 0.0118 - acc 0.9976\n",
            "EPOCH 7 (84/112) - loss 0.0119 - acc 0.9976\n",
            "EPOCH 7 (98/112) - loss 0.0118 - acc 0.9975\n",
            "EPOCH 7 (112/112) - loss 0.0117 - acc 0.9976\n",
            "EPOCH 7 - loss 0.0117 - acc 0.9976 ---------------------- \n",
            "EPOCH 8 (14/112) - loss 0.0129 - acc 0.9978\n",
            "EPOCH 8 (28/112) - loss 0.0121 - acc 0.9978\n",
            "EPOCH 8 (42/112) - loss 0.0119 - acc 0.9978\n",
            "EPOCH 8 (56/112) - loss 0.0119 - acc 0.9976\n",
            "EPOCH 8 (70/112) - loss 0.0118 - acc 0.9977\n",
            "EPOCH 8 (84/112) - loss 0.0117 - acc 0.9977\n",
            "EPOCH 8 (98/112) - loss 0.0117 - acc 0.9977\n",
            "EPOCH 8 (112/112) - loss 0.0115 - acc 0.9977\n",
            "EPOCH 8 - loss 0.0115 - acc 0.9977 ---------------------- \n",
            "EPOCH 9 (14/112) - loss 0.0117 - acc 0.9978\n",
            "EPOCH 9 (28/112) - loss 0.0110 - acc 0.9978\n",
            "EPOCH 9 (42/112) - loss 0.0112 - acc 0.9978\n",
            "EPOCH 9 (56/112) - loss 0.0109 - acc 0.9978\n",
            "EPOCH 9 (70/112) - loss 0.0111 - acc 0.9977\n",
            "EPOCH 9 (84/112) - loss 0.0108 - acc 0.9978\n",
            "EPOCH 9 (98/112) - loss 0.0110 - acc 0.9978\n",
            "EPOCH 9 (112/112) - loss 0.0110 - acc 0.9978\n",
            "EPOCH 9 - loss 0.0110 - acc 0.9978 ---------------------- \n",
            "EPOCH 10 (14/112) - loss 0.0098 - acc 0.9982\n",
            "EPOCH 10 (28/112) - loss 0.0104 - acc 0.9978\n",
            "EPOCH 10 (42/112) - loss 0.0102 - acc 0.9979\n",
            "EPOCH 10 (56/112) - loss 0.0103 - acc 0.9979\n",
            "EPOCH 10 (70/112) - loss 0.0102 - acc 0.9979\n",
            "EPOCH 10 (84/112) - loss 0.0107 - acc 0.9978\n",
            "EPOCH 10 (98/112) - loss 0.0108 - acc 0.9978\n",
            "EPOCH 10 (112/112) - loss 0.0107 - acc 0.9978\n",
            "EPOCH 10 - loss 0.0107 - acc 0.9978 ---------------------- \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ITvK9tBNyq4F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'drive/My Drive/Colab Notebooks/models/gpt_machado.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FpJ1R8U94SKq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load('drive/My Drive/Colab Notebooks/models/gpt_machado.pt')\n",
        "model.load_state_dict(checkpoint)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7FE8x9fXEc_C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "c9918834-0d6b-44a7-e58c-4431145ecb07"
      },
      "cell_type": "code",
      "source": [
        "# inference\n",
        "seed = 'ojos azules como el agua \\n almas solas en tormento \\n la calle desierta \\n ¡ un amor ardiente ! \\n'\n",
        "seed = [word_idx[word] for word in seed.split(' ')]\n",
        "\n",
        "def sample(softmax_logits, temperature=1.0):\n",
        "    tempered = F.softmax(softmax_logits / temperature, dim=-1)\n",
        "    distribution = torch.multinomial(tempered, 1)\n",
        "    return distribution[0]\n",
        "\n",
        "model.cuda()\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i in range(300):\n",
        "        x = torch.from_numpy(np.array(seed)).unsqueeze(0).cuda()\n",
        "        mask = transformer.gen_target_mask(x.cpu(), 0).cuda()\n",
        "        encoded = model.encoder(x, mask)\n",
        "        out = model.out(encoded)\n",
        "        \n",
        "        #out_softmax = F.softmax(out, dim=-1)\n",
        "        # idx = torch.argmax(out_softmax, dim=-1)[:, -1].item()\n",
        "        \n",
        "        idx = sample(out[:, -1], temperature=0.7).item()\n",
        "        \n",
        "        seed.append(idx)\n",
        "        if idx_word == 0:\n",
        "            break\n",
        "        word = idx_word[idx]\n",
        "        if word == '<end>':\n",
        "            break\n",
        "\n",
        "print(' '.join([idx_word[idx] for idx in seed]))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ojos azules como el agua \n",
            " almas solas en tormento \n",
            " la calle desierta \n",
            " ¡ un amor ardiente ! \n",
            " cerca del duero arriba . \n",
            " la voz que la mar se le ilumina , \n",
            " la torre castellana . \n",
            " la hora , \n",
            " sólo el río , \n",
            " junto al agua que hizo un « algunas nubes plomizas \n",
            " como un jardín sombrío , \n",
            " como un « porque aguardo y el puro , \n",
            " hacia la fuente . \n",
            " hacia la tierra sombrío \n",
            " ya sólo el pan con otras doncellitas \n",
            " en los días azules y , \n",
            " cuando crecen las blancas margaritas . \n",
            " vi \n",
            " ¡ oh dulce y la mar ! \n",
            " cabeza de extremadura , \n",
            " con su poma , \n",
            " arruinado , \n",
            " estos limonares verdes \n",
            " con sus hojas , de sus casas denegridas ! \n",
            " ¡ oh , \n",
            " ii \n",
            " y es del agua limpia : \n",
            " la oveja pace , \n",
            " por ti y siempre espera , \n",
            " en sus galgos , \n",
            " de españa , que tiene \n",
            " en torno de fe sin verdor ; \n",
            " con tu tronco ceniciento \n",
            " sin esbeltez ni canciones \n",
            " cuando graznan las cornejas ! \n",
            " y siempre o aparece \n",
            " como tus largos ríos , \n",
            " hacia la mar ! \n",
            " soria , sobrado lacónico . \n",
            " ¿ acaso os asombra mi corazón ! \n",
            " de vanidades , \n",
            " ¿ espera , \n",
            " v \n",
            " es el criador y la criatura lo hace ; \n",
            " de ballesta \n",
            " en torno a soria , obscuros encinares , \n",
            " ariscos pedregales , calvas sierras , \n",
            " caminos , \n",
            " labios que es vivir como se puede . \n",
            " del rosario . \n",
            " día y riel . \n",
            " cómo son los liberales \n",
            " de un corazón blasfemo . \n",
            " con doble faz de amor el hielo invernizo , \n",
            " cerca del alto\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sUFR3g1WEc_g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}