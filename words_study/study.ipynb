{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to make a quick study into how a neural network can input & output **words**\n",
    "\n",
    "We will explore this types of representation:\n",
    "- One Hot encoding\n",
    "- Raw integer\n",
    "- Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!! The dataset contains word pairs with multiple possible outputs, for example, 'abalanzar' will be associated with multiple synonims so we will just pick the first one for the sake of simplicity (unless adding a random noise to the inpiut, the network will have a very hard time trying to associate a single input with multiple possible outputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair count: 4502\n",
      "Word count: 4502\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['abalanzar', 'equilibrar'],\n",
       " ['equilibrar', 'abalanzar'],\n",
       " ['abecedario', 'silabario'],\n",
       " ['silabario', 'abecedario'],\n",
       " ['abertura', 'rendija'],\n",
       " ['rendija', 'abertura'],\n",
       " ['ablandar', 'molificar'],\n",
       " ['molificar', 'ablandar'],\n",
       " ['abogar', 'patrocinar'],\n",
       " ['patrocinar', 'abogar']]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# first let's load the data\n",
    "\n",
    "pairs = []\n",
    "words = []\n",
    "\n",
    "with open(\"sinonimos.txt\", \"r\") as document:\n",
    "    for line in document:\n",
    "        raw_string = re.sub(r'[^\\w\\s]', '', line)\n",
    "        pair = raw_string.split()[:2]\n",
    "        if not pair[0] in words and not pair[1] in words:\n",
    "            pairs.append(pair)\n",
    "            pairs.append(pair[::-1])\n",
    "            words.append(pair[0])\n",
    "            words.append(pair[1])\n",
    "        \n",
    "print(\"Pair count:\", len(pairs))\n",
    "print(\"Word count:\", len(words))\n",
    "pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: 4502\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('abalanzar', 1),\n",
       " ('equilibrar', 1),\n",
       " ('abecedario', 1),\n",
       " ('silabario', 1),\n",
       " ('abertura', 1),\n",
       " ('rendija', 1),\n",
       " ('ablandar', 1),\n",
       " ('molificar', 1),\n",
       " ('abogar', 1),\n",
       " ('patrocinar', 1),\n",
       " ('abolir', 1),\n",
       " ('derogar', 1),\n",
       " ('abominar', 1),\n",
       " ('detestar', 1),\n",
       " ('aborigen', 1),\n",
       " ('nativo', 1),\n",
       " ('abortar', 1),\n",
       " ('malparir', 1),\n",
       " ('abrasar', 1),\n",
       " ('quemar', 1),\n",
       " ('abrazar', 1),\n",
       " ('ce√±ir', 1),\n",
       " ('abrir', 1),\n",
       " ('perforar', 1),\n",
       " ('absorber', 1),\n",
       " ('embeber', 1),\n",
       " ('abstenerse', 1),\n",
       " ('privarse', 1),\n",
       " ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_counter = Counter(words)\n",
    "print(\"Unique words:\", len(word_counter.most_common()))\n",
    "word_counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:  4500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[3, 4],\n",
       " [4, 3],\n",
       " [5, 6],\n",
       " [6, 5],\n",
       " [7, 8],\n",
       " [8, 7],\n",
       " [9, 10],\n",
       " [10, 9],\n",
       " [11, 12],\n",
       " [12, 11]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# Let's now create a word dictionary\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(pairs)\n",
    "\n",
    "tokenized_pairs = tokenizer.texts_to_sequences(pairs)\n",
    "\n",
    "print(\"Number of words: \", len(tokenizer.word_index))\n",
    "tokenized_pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['patrocinar', 'abominar', 'acrecentar', 'abrasar']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will help us visualize the outputs much better\n",
    "def indexToWord(indices):\n",
    "    words = []\n",
    "    for idx in indices:\n",
    "        words.append(tokenizer.index_word[idx])\n",
    "    return words\n",
    "\n",
    "indexToWord([12, 15, 34, 21])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in: One Hot - out: One Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4502, 4501) (4502, 4501)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['abalanzar', 'equilibrar']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "one_hot_pairs = to_categorical(tokenized_pairs)\n",
    "\n",
    "one_hot_x = one_hot_pairs[:, 0] \n",
    "one_hot_y = one_hot_pairs[:, 1]\n",
    "\n",
    "print(one_hot_x.shape, one_hot_y.shape)\n",
    "indexToWord([np.argmax(one_hot_x[0]), np.argmax(one_hot_y[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 300)               1350600   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4501)              1354801   \n",
      "=================================================================\n",
      "Total params: 2,705,401\n",
      "Trainable params: 2,705,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "4502/4502 [==============================] - 3s 604us/step - loss: 8.4186 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "4502/4502 [==============================] - 3s 680us/step - loss: 8.3847 - acc: 0.4138\n",
      "Epoch 3/5\n",
      "4502/4502 [==============================] - 2s 518us/step - loss: 8.3271 - acc: 0.7601\n",
      "Epoch 4/5\n",
      "4502/4502 [==============================] - 2s 533us/step - loss: 8.2387 - acc: 0.8256\n",
      "Epoch 5/5\n",
      "4502/4502 [==============================] - 3s 615us/step - loss: 8.1263 - acc: 0.8903\n",
      "Expected: ['equilibrar', 'infortunio', 'consciente', 'apariencia']\n",
      "Predicted: ['equilibrar', 'infortunio', 'consciente', 'apariencia']\n"
     ]
    }
   ],
   "source": [
    "#Let's now do a very simple model that will try to learn using One Hot input and output\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(300, activation=\"relu\", input_shape=(4501,)))\n",
    "model.add(Dense(4501, activation=\"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(one_hot_x, one_hot_y, shuffle=True, epochs=5, batch_size=256)\n",
    "\n",
    "# Test trained model\n",
    "tokenized_y = np.array(tokenized_pairs)[:, 1]\n",
    "expected = indexToWord([tokenized_y[0], tokenized_y[100], tokenized_y[200], tokenized_y[300]])\n",
    "\n",
    "pred = model.predict(np.array([one_hot_x[0], one_hot_x[100], one_hot_x[200], one_hot_x[300]]))\n",
    "pred_tokenized = [np.argmax(p) for p in pred]\n",
    "pred_words = indexToWord(pred_tokenized)\n",
    "\n",
    "print(\"Expected:\", expected)\n",
    "print(\"Predicted:\", pred_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that by using one hot encoddings the model can learn but after 5 epochs it only achieves 65% accuracy, this it's to be expected as a vector of size 4501 with all zeros except for a single 1 has a VERY low activation power and it's very hard for it to learn... :(  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in: Raw integer - out: One Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 300)               600       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4501)              1354801   \n",
      "=================================================================\n",
      "Total params: 1,355,401\n",
      "Trainable params: 1,355,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "4502/4502 [==============================] - 2s 486us/step - loss: 15.9462 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "4502/4502 [==============================] - 2s 437us/step - loss: 15.8160 - acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "4502/4502 [==============================] - 2s 383us/step - loss: 15.7919 - acc: 0.0000e+00\n",
      "Epoch 4/5\n",
      "4502/4502 [==============================] - 1s 332us/step - loss: 15.7793 - acc: 2.2212e-04\n",
      "Epoch 5/5\n",
      "4502/4502 [==============================] - 1s 330us/step - loss: 15.7803 - acc: 0.0000e+00\n",
      "Expected: ['equilibrar', 'infortunio', 'consciente', 'apariencia']\n",
      "Predicted: ['aducir', 'apartamento', 'apartamento', 'apartamento']\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = Sequential()\n",
    "model.add(Dense(300, activation=\"relu\", input_shape=(1,))) # same as before, but it accepts a single value as input: the raw token\n",
    "model.add(Dense(4501, activation=\"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "tokenized_x = np.array(tokenized_pairs)[:, 0]\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(tokenized_x, one_hot_y, shuffle=True, epochs=5, batch_size=256)\n",
    "\n",
    "##¬†Test trained model\n",
    "pred = model.predict(np.array([tokenized_x[0], tokenized_x[100], tokenized_x[200], tokenized_x[300]]))\n",
    "pred_tokenized = [np.argmax(p) for p in pred]\n",
    "pred_words = indexToWord(pred_tokenized)\n",
    "\n",
    "print(\"Expected:\", expected)\n",
    "print(\"Predicted:\", pred_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the input is a raw integer the model doesn't seem to learn anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in: Embeddings - out: One Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1, 100)            450100    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 300)               30300     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4501)              1354801   \n",
      "=================================================================\n",
      "Total params: 1,835,201\n",
      "Trainable params: 1,835,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "4502/4502 [==============================] - 2s 521us/step - loss: 8.4182 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "4502/4502 [==============================] - 2s 436us/step - loss: 8.3891 - acc: 0.2692\n",
      "Epoch 3/5\n",
      "4502/4502 [==============================] - 2s 387us/step - loss: 8.3343 - acc: 0.5267\n",
      "Epoch 4/5\n",
      "4502/4502 [==============================] - 2s 392us/step - loss: 8.2231 - acc: 0.5506\n",
      "Epoch 5/5\n",
      "4502/4502 [==============================] - 2s 396us/step - loss: 8.0313 - acc: 0.6568\n",
      "Expected: ['equilibrar', 'infortunio', 'consciente', 'apariencia']\n",
      "Predicted: ['equilibrar', 'infortunio', 'consciente', 'apariencia']\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, Reshape\n",
    "# Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(4501, 100, input_length=1)) # the input shape is the same, but we transform it to an embedding before passing it further\n",
    "model.add(Reshape((100,)))\n",
    "model.add(Dense(300, activation=\"relu\"))\n",
    "model.add(Dense(4501, activation=\"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "tokenized_x = np.array(tokenized_pairs)[:, 0]\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(tokenized_x, one_hot_y, shuffle=True, epochs=5, batch_size=256)\n",
    "\n",
    "##¬†Test trained model\n",
    "pred = model.predict(np.array([tokenized_x[0], tokenized_x[100], tokenized_x[200], tokenized_x[300]]))\n",
    "pred_tokenized = [np.argmax(p) for p in pred]\n",
    "pred_words = indexToWord(pred_tokenized)\n",
    "\n",
    "print(\"Expected:\", expected)\n",
    "print(\"Predicted:\", pred_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so good :( only 42% accuracy\n",
    "Maybe the big bottleneck here is the output... let's start fiddling with that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in: One Hot - out: Raw integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_84 (Dense)             (None, 300)               1350600   \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 1,350,901\n",
      "Trainable params: 1,350,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "4502/4502 [==============================] - 2s 508us/step - loss: 6748634.2079 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "4502/4502 [==============================] - 1s 211us/step - loss: 6746899.4187 - acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "4502/4502 [==============================] - 1s 165us/step - loss: 6744029.1790 - acc: 4.4425e-04\n",
      "Epoch 4/5\n",
      "4502/4502 [==============================] - 1s 155us/step - loss: 6739843.4809 - acc: 4.4425e-04\n",
      "Epoch 5/5\n",
      "4502/4502 [==============================] - 1s 171us/step - loss: 6734239.3632 - acc: 4.4425e-04\n",
      "[4, 4, 4, 4]\n",
      "Expected: ['equilibrar', 'infortunio', 'consciente', 'apariencia']\n",
      "Predicted: ['equilibrar', 'equilibrar', 'equilibrar', 'equilibrar']\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, activation=\"relu\", input_shape=(4501,)))\n",
    "model.add(Dense(1)) # we need the outputs to be values so we'll use a relu activation\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(one_hot_x, tokenized_y, epochs=5, batch_size=256)\n",
    "\n",
    "# Test trained model\n",
    "expected = indexToWord([tokenized_y[0], tokenized_y[100], tokenized_y[200], tokenized_y[300]])\n",
    "\n",
    "pred = model.predict(np.array([one_hot_x[0], one_hot_x[100], one_hot_x[200], one_hot_x[300]]))\n",
    "pred_tokenized = [x[0] for x in np.rint(pred).astype(np.int16)]\n",
    "print(pred_tokenized)\n",
    "pred_words = indexToWord(pred_tokenized)\n",
    "\n",
    "print(\"Expected:\", expected)\n",
    "print(\"Predicted:\", pred_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a raw integer as output also doesn't work :( the model has a very high loss that seems to be reducing but very VERY slowly, also, the model risks of outputing the wrong word just because thge value was 34.4 instead of 34.6 when the expected value is 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in: One Hot - out: Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_88 (Dense)             (None, 300)               1350600   \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 4501)              1354801   \n",
      "=================================================================\n",
      "Total params: 2,705,401\n",
      "Trainable params: 2,705,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "4502/4502 [==============================] - 3s 729us/step - loss: 8.4185 - acc: 2.2212e-04\n",
      "Epoch 2/5\n",
      "4502/4502 [==============================] - 2s 456us/step - loss: 8.3842 - acc: 0.4060\n",
      "Epoch 3/5\n",
      "4502/4502 [==============================] - 2s 455us/step - loss: 8.3263 - acc: 0.5538\n",
      "Epoch 4/5\n",
      "4502/4502 [==============================] - 2s 462us/step - loss: 8.2375 - acc: 0.2750\n",
      "Epoch 5/5\n",
      "4502/4502 [==============================] - 2s 454us/step - loss: 8.1253 - acc: 0.5860\n",
      "Expected: ['equilibrar', 'infortunio', 'consciente', 'apariencia']\n",
      "Predicted: ['equilibrar', 'infortunio', 'consciente', 'apariencia']\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, activation=\"relu\", input_shape=(4501,)))\n",
    "model.add(Dense(4501, activation=\"softmax\")) # the model will output a one-hot encoded value, but thanks to the loss function it will be associated with an embedding\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(one_hot_x, tokenized_y, shuffle=True, epochs=5, batch_size=256)\n",
    "\n",
    "# Test trained model\n",
    "tokenized_y = np.array(tokenized_pairs)[:, 1]\n",
    "expected = indexToWord([tokenized_y[0], tokenized_y[100], tokenized_y[200], tokenized_y[300]])\n",
    "\n",
    "pred = model.predict(np.array([one_hot_x[0], one_hot_x[100], one_hot_x[200], one_hot_x[300]]))\n",
    "pred_tokenized = [np.argmax(p) for p in pred]\n",
    "pred_words = indexToWord(pred_tokenized)\n",
    "\n",
    "print(\"Expected:\", expected)\n",
    "print(\"Predicted:\", pred_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performs very similarly to the One Hot - One Hot model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in: Embeddings - out: Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1, 200)            900200    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 300)               60300     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4501)              1354801   \n",
      "=================================================================\n",
      "Total params: 2,315,301\n",
      "Trainable params: 2,315,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(4502,) (4502,)\n",
      "Epoch 1/15\n",
      "4502/4502 [==============================] - 2s 477us/step - loss: 8.4192 - acc: 0.0000e+00\n",
      "Epoch 2/15\n",
      "4502/4502 [==============================] - 1s 330us/step - loss: 8.3784 - acc: 0.4016\n",
      "Epoch 3/15\n",
      "4502/4502 [==============================] - 1s 327us/step - loss: 8.3038 - acc: 0.7379\n",
      "Epoch 4/15\n",
      "4502/4502 [==============================] - 2s 351us/step - loss: 8.1536 - acc: 0.8625\n",
      "Epoch 5/15\n",
      "4502/4502 [==============================] - 2s 423us/step - loss: 7.8922 - acc: 0.9191\n",
      "Epoch 6/15\n",
      "4502/4502 [==============================] - 1s 299us/step - loss: 7.4757 - acc: 0.9067\n",
      "Epoch 7/15\n",
      "4502/4502 [==============================] - 1s 297us/step - loss: 6.8488 - acc: 0.9936\n",
      "Epoch 8/15\n",
      "4502/4502 [==============================] - 2s 411us/step - loss: 5.9618 - acc: 0.9996\n",
      "Epoch 9/15\n",
      "4502/4502 [==============================] - 2s 375us/step - loss: 4.7690 - acc: 0.9996\n",
      "Epoch 10/15\n",
      "4502/4502 [==============================] - 2s 352us/step - loss: 3.2564 - acc: 0.9993\n",
      "Epoch 11/15\n",
      "4502/4502 [==============================] - 2s 445us/step - loss: 1.6005 - acc: 0.9993\n",
      "Epoch 12/15\n",
      "4502/4502 [==============================] - 1s 307us/step - loss: 0.4749 - acc: 0.9993\n",
      "Epoch 13/15\n",
      "4502/4502 [==============================] - 2s 356us/step - loss: 0.1407 - acc: 0.9993\n",
      "Epoch 14/15\n",
      "4502/4502 [==============================] - 2s 352us/step - loss: 0.0700 - acc: 0.9996\n",
      "Epoch 15/15\n",
      "4502/4502 [==============================] - 1s 308us/step - loss: 0.0478 - acc: 0.9996\n",
      "Expected: ['equilibrar', 'infortunio', 'consciente', 'apariencia']\n",
      "Predicted: ['equilibrar', 'infortunio', 'consciente', 'apariencia']\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(4501, 200, input_length=1)) # the input shape is the same, but we transform it to an embedding before passing it further\n",
    "model.add(Reshape((200,)))\n",
    "model.add(Dense(300, activation=\"relu\"))\n",
    "model.add(Dense(4501, activation=\"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "tokenized_x = np.array(tokenized_pairs)[:, 0]\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "print(tokenized_x.shape, tokenized_y.shape)\n",
    "model.fit(tokenized_x, tokenized_y, epochs=15, batch_size=256)\n",
    "\n",
    "##¬†Test trained model\n",
    "pred = model.predict(np.array([tokenized_x[0], tokenized_x[100], tokenized_x[200], tokenized_x[300]]))\n",
    "pred_tokenized = [np.argmax(p) for p in pred]\n",
    "pred_words = indexToWord(pred_tokenized)\n",
    "\n",
    "print(\"Expected:\", expected)\n",
    "print(\"Predicted:\", pred_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is definitely the best model, as we achieved 82% accuracy after only 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precisi√≥n   ['exactitud']\n"
     ]
    }
   ],
   "source": [
    "word = 'precisi√≥n'\n",
    "word_index = tokenizer.word_index[word]\n",
    "synonim = model.predict(np.array([word_index]))[0]\n",
    "synonim = indexToWord([np.argmax(synonim)])\n",
    "\n",
    "print(word, \" \", synonim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
