{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to make a quick study into how a neural network can input & output **words**\n",
    "\n",
    "We will explore this types of representation:\n",
    "- One Hot encoding\n",
    "- Raw integer\n",
    "- Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!! The dataset contains word pairs with multiple possible outputs, for example, 'abalanzar' will be associated with multiple synonims so we will just pick the first one for the sake of simplicity (unless adding a random noise to the inpiut, the network will have a very hard time trying to associate a single input with multiple possible outputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair count: 4502\n",
      "Word count: 4502\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['abalanzar', 'equilibrar'],\n",
       " ['equilibrar', 'abalanzar'],\n",
       " ['abecedario', 'silabario'],\n",
       " ['silabario', 'abecedario'],\n",
       " ['abertura', 'rendija'],\n",
       " ['rendija', 'abertura'],\n",
       " ['ablandar', 'molificar'],\n",
       " ['molificar', 'ablandar'],\n",
       " ['abogar', 'patrocinar'],\n",
       " ['patrocinar', 'abogar']]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# first let's load the data\n",
    "\n",
    "pairs = []\n",
    "words = []\n",
    "\n",
    "with open(\"sinonimos.txt\", \"r\") as document:\n",
    "    for line in document:\n",
    "        raw_string = re.sub(r'[^\\w\\s]', '', line)\n",
    "        pair = raw_string.split()[:2]\n",
    "        if not pair[0] in words and not pair[1] in words:\n",
    "            pairs.append(pair)\n",
    "            pairs.append(pair[::-1])\n",
    "            words.append(pair[0])\n",
    "            words.append(pair[1])\n",
    "        \n",
    "print(\"Pair count:\", len(pairs))\n",
    "print(\"Word count:\", len(words))\n",
    "pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: 4502\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('abalanzar', 1),\n",
       " ('equilibrar', 1),\n",
       " ('abecedario', 1),\n",
       " ('silabario', 1),\n",
       " ('abertura', 1),\n",
       " ('rendija', 1),\n",
       " ('ablandar', 1),\n",
       " ('molificar', 1),\n",
       " ('abogar', 1),\n",
       " ('patrocinar', 1),\n",
       " ('abolir', 1),\n",
       " ('derogar', 1),\n",
       " ('abominar', 1),\n",
       " ('detestar', 1),\n",
       " ('aborigen', 1),\n",
       " ('nativo', 1),\n",
       " ('abortar', 1),\n",
       " ('malparir', 1),\n",
       " ('abrasar', 1),\n",
       " ('quemar', 1),\n",
       " ('abrazar', 1),\n",
       " ('ceñir', 1),\n",
       " ('abrir', 1),\n",
       " ('perforar', 1),\n",
       " ('absorber', 1),\n",
       " ('embeber', 1),\n",
       " ('abstenerse', 1),\n",
       " ('privarse', 1),\n",
       " ('abstracción', 1),\n",
       " ('ensimismamiento', 1),\n",
       " ('abultar', 1),\n",
       " ('acrecentar', 1),\n",
       " ('abundar', 1),\n",
       " ('sobrar', 1),\n",
       " ('abusar', 1),\n",
       " ('atropellar', 1),\n",
       " ('abyección', 1),\n",
       " ('Infamia', 1),\n",
       " ('abyecto', 1),\n",
       " ('despreciable', 1),\n",
       " ('acabar', 1),\n",
       " ('terminar', 1),\n",
       " ('academia', 1),\n",
       " ('escuela', 1),\n",
       " ('acaecer', 1),\n",
       " ('suceder', 1),\n",
       " ('acalorarse', 1),\n",
       " ('exaltarse', 1),\n",
       " ('acatar', 1),\n",
       " ('obedecer', 1),\n",
       " ('acción', 1),\n",
       " ('hecho', 1),\n",
       " ('acendrar', 1),\n",
       " ('depurar', 1),\n",
       " ('acervo', 1),\n",
       " ('cúmulo', 1),\n",
       " ('achaque', 1),\n",
       " ('dolencia', 1),\n",
       " ('acibarar', 1),\n",
       " ('amargar', 1),\n",
       " ('aclamar', 1),\n",
       " ('aplaudir', 1),\n",
       " ('aclarar', 1),\n",
       " ('explicar', 1),\n",
       " ('acodar', 1),\n",
       " ('sostener', 1),\n",
       " ('acollar', 1),\n",
       " ('uncir', 1),\n",
       " ('acomodar', 1),\n",
       " ('adecuar', 1),\n",
       " ('acoplar', 1),\n",
       " ('unir', 1),\n",
       " ('acorazar', 1),\n",
       " ('blindar', 1),\n",
       " ('acribillar', 1),\n",
       " ('agujerear', 1),\n",
       " ('actitud', 1),\n",
       " ('ademán', 1),\n",
       " ('actual', 1),\n",
       " ('presente', 1),\n",
       " ('actuar', 1),\n",
       " ('obrar', 1),\n",
       " ('acuñar', 1),\n",
       " ('amonedar', 1),\n",
       " ('además', 1),\n",
       " ('también', 1),\n",
       " ('adherir', 1),\n",
       " ('aceptar', 1),\n",
       " ('adjudicar', 1),\n",
       " ('asignar', 1),\n",
       " ('administrar', 1),\n",
       " ('dirigir', 1),\n",
       " ('adobar', 1),\n",
       " ('aliñar', 1),\n",
       " ('adopción', 1),\n",
       " ('aceptación', 1),\n",
       " ('adosar', 1),\n",
       " ('arrimar', 1),\n",
       " ('adular', 1),\n",
       " ('lisonjear', 1),\n",
       " ('adversidad', 1),\n",
       " ('infortunio', 1),\n",
       " ('afán', 1),\n",
       " ('anhelo', 1),\n",
       " ('aferrar', 1),\n",
       " ('asir', 1),\n",
       " ('afilar', 1),\n",
       " ('aguzar', 1),\n",
       " ('aflorar', 1),\n",
       " ('asomar', 1),\n",
       " ('aforismo', 1),\n",
       " ('sentencia', 1),\n",
       " ('afrentar', 1),\n",
       " ('agraviar', 1),\n",
       " ('ágape', 1),\n",
       " ('atención', 1),\n",
       " ('agasajar', 1),\n",
       " ('festejar', 1),\n",
       " ('agitar', 1),\n",
       " ('conmover', 1),\n",
       " ('agnación', 1),\n",
       " ('parentesco', 1),\n",
       " ('agorar', 1),\n",
       " ('presagiar', 1),\n",
       " ('agravar', 1),\n",
       " ('empeorar', 1),\n",
       " ('agregar', 1),\n",
       " ('añadir', 1),\n",
       " ('ajar', 1),\n",
       " ('marchitar', 1),\n",
       " ('ajusticiar', 1),\n",
       " ('ejecutar', 1),\n",
       " ('alambicar', 1),\n",
       " ('sutilizar', 1),\n",
       " ('alborozar', 1),\n",
       " ('regocijar', 1),\n",
       " ('alegar', 1),\n",
       " ('aducir', 1),\n",
       " ('algarabía', 1),\n",
       " ('griterío', 1),\n",
       " ('alienar', 1),\n",
       " ('enajenar', 1),\n",
       " ('aliviar', 1),\n",
       " ('moderar', 1),\n",
       " ('allanar', 1),\n",
       " ('facilitar', 1),\n",
       " ('alquilar', 1),\n",
       " ('arrendar', 1),\n",
       " ('alterar', 1),\n",
       " ('cambiar', 1),\n",
       " ('altibajo', 1),\n",
       " ('fluctuación', 1),\n",
       " ('altruista', 1),\n",
       " ('generoso', 1),\n",
       " ('aluvión', 1),\n",
       " ('inundación', 1),\n",
       " ('alzamiento', 1),\n",
       " ('sublevación', 1),\n",
       " ('amagar', 1),\n",
       " ('amenazar', 1),\n",
       " ('amante', 1),\n",
       " ('galante', 1),\n",
       " ('amaño', 1),\n",
       " ('treta', 1),\n",
       " ('ambages', 1),\n",
       " ('rodeos', 1),\n",
       " ('amenidad', 1),\n",
       " ('deleite', 1),\n",
       " ('amenizar', 1),\n",
       " ('encantar', 1),\n",
       " ('aminorar', 1),\n",
       " ('disminuir', 1),\n",
       " ('amodorramiento', 1),\n",
       " ('somnolencia', 1),\n",
       " ('amplitud', 1),\n",
       " ('extensión', 1),\n",
       " ('anáfora', 1),\n",
       " ('repetición', 1),\n",
       " ('andanada', 1),\n",
       " ('descarga', 1),\n",
       " ('andrajoso', 1),\n",
       " ('harapiento', 1),\n",
       " ('anodino', 1),\n",
       " ('insubstancial', 1),\n",
       " ('anomalía', 1),\n",
       " ('anormalidad', 1),\n",
       " ('ansiedad', 1),\n",
       " ('angustia', 1),\n",
       " ('anteceder', 1),\n",
       " ('preceder', 1),\n",
       " ('antelación', 1),\n",
       " ('anticipación', 1),\n",
       " ('anticonstitucional', 1),\n",
       " ('ilegal', 1),\n",
       " ('antídoto', 1),\n",
       " ('contraveneno', 1),\n",
       " ('antitético', 1),\n",
       " ('opuesto', 1),\n",
       " ('antonomasia', 1),\n",
       " ('excelencia', 1),\n",
       " ('anuente', 1),\n",
       " ('consciente', 1),\n",
       " ('anunciar', 1),\n",
       " ('avisar', 1),\n",
       " ('añagaza', 1),\n",
       " ('señuelo', 1),\n",
       " ('apabullar', 1),\n",
       " ('aplastar', 1),\n",
       " ('apacible', 1),\n",
       " ('tranquilo', 1),\n",
       " ('aparición', 1),\n",
       " ('visión', 1),\n",
       " ('apartamento', 1),\n",
       " ('vivienda', 1),\n",
       " ('apartamiento', 1),\n",
       " ('separación', 1),\n",
       " ('apasionar', 1),\n",
       " ('exaltar', 1),\n",
       " ('apego', 1),\n",
       " ('inclinación', 1),\n",
       " ('apéndice', 1),\n",
       " ('agregado', 1),\n",
       " ('apertura', 1),\n",
       " ('inauguración', 1),\n",
       " ('apetito', 1),\n",
       " ('deseo', 1),\n",
       " ('apiolar', 1),\n",
       " ('prender', 1),\n",
       " ('aplacar', 1),\n",
       " ('calmar', 1),\n",
       " ('aplazar', 1),\n",
       " ('postergar', 1),\n",
       " ('apocar', 1),\n",
       " ('minorar', 1),\n",
       " ('apogeo', 1),\n",
       " ('plenitud', 1),\n",
       " ('apostasía', 1),\n",
       " ('abjuración', 1),\n",
       " ('apostatar', 1),\n",
       " ('renegar', 1),\n",
       " ('apostrofar', 1),\n",
       " ('achacar', 1),\n",
       " ('apoteósico', 1),\n",
       " ('apoteótico', 1),\n",
       " ('apreciación', 1),\n",
       " ('evaluación', 1),\n",
       " ('aprensivo', 1),\n",
       " ('receloso', 1),\n",
       " ('apresurar', 1),\n",
       " ('aligerar', 1),\n",
       " ('apretado', 1),\n",
       " ('mezquino', 1),\n",
       " ('aprovisionamiento', 1),\n",
       " ('abastecimiento', 1),\n",
       " ('aproximar', 1),\n",
       " ('acercar', 1),\n",
       " ('aptitud', 1),\n",
       " ('idoneidad', 1),\n",
       " ('aquejar', 1),\n",
       " ('afligir', 1),\n",
       " ('aquelarre', 1),\n",
       " ('consentimiento', 1),\n",
       " ('arbitrariedad', 1),\n",
       " ('abuso', 1),\n",
       " ('arcaico', 1),\n",
       " ('antiguo', 1),\n",
       " ('arduo', 1),\n",
       " ('difícil', 1),\n",
       " ('aristócrata', 1),\n",
       " ('noble', 1),\n",
       " ('arraigar', 1),\n",
       " ('enraizar', 1),\n",
       " ('arreciar', 1),\n",
       " ('aumentar', 1),\n",
       " ('arrepticio', 1),\n",
       " ('endemoniado', 1),\n",
       " ('arriar', 1),\n",
       " ('bajar', 1),\n",
       " ('arriesgar', 1),\n",
       " ('exponer', 1),\n",
       " ('arritmia', 1),\n",
       " ('irregularidad', 1),\n",
       " ('arropar', 1),\n",
       " ('abrigar', 1),\n",
       " ('arruinar', 1),\n",
       " ('arrasar', 1),\n",
       " ('arsenal', 1),\n",
       " ('depósito', 1),\n",
       " ('artífice', 1),\n",
       " ('creador', 1),\n",
       " ('asaz', 1),\n",
       " ('suficiente', 1),\n",
       " ('ascendiente', 1),\n",
       " ('influencia', 1),\n",
       " ('asentir', 1),\n",
       " ('aprobar', 1),\n",
       " ('aseverar', 1),\n",
       " ('afirmar', 1),\n",
       " ('asiduidad', 1),\n",
       " ('frecuencia', 1),\n",
       " ('aspecto', 1),\n",
       " ('apariencia', 1),\n",
       " ('astenia', 1),\n",
       " ('debilidad', 1),\n",
       " ('astucia', 1),\n",
       " ('picardía', 1),\n",
       " ('asumir', 1),\n",
       " ('ocupar', 1),\n",
       " ('asustar', 1),\n",
       " ('espantar', 1),\n",
       " ('atañer', 1),\n",
       " ('concernir', 1),\n",
       " ('atascar', 1),\n",
       " ('atorar', 1),\n",
       " ('ataviar', 1),\n",
       " ('acicalar', 1),\n",
       " ('atentar', 1),\n",
       " ('delinquir', 1),\n",
       " ('atisbar', 1),\n",
       " ('mirar', 1),\n",
       " ('atizado', 1),\n",
       " ('tostado', 1),\n",
       " ('atizar', 1),\n",
       " ('avivar', 1),\n",
       " ('atónito', 1),\n",
       " ('estupefacto', 1),\n",
       " ('atosigar', 1),\n",
       " ('abrumar', 1),\n",
       " ('atrasar', 1),\n",
       " ('demorar', 1),\n",
       " ('atribular', 1),\n",
       " ('angustiar', 1),\n",
       " ('atrofiar', 1),\n",
       " ('padecer', 1),\n",
       " ('atroz', 1),\n",
       " ('fiero', 1),\n",
       " ('audacia', 1),\n",
       " ('valor', 1),\n",
       " ('auditor', 1),\n",
       " ('oyente', 1),\n",
       " ('áulico', 1),\n",
       " ('palaciego', 1),\n",
       " ('autárquico', 1),\n",
       " ('autoabastecido', 1),\n",
       " ('autómata', 1),\n",
       " ('manejable', 1),\n",
       " ('autoridad', 1),\n",
       " ('poder', 1),\n",
       " ('avatar', 1),\n",
       " ('cambio', 1),\n",
       " ('avería', 1),\n",
       " ('daño', 1),\n",
       " ('aversión', 1),\n",
       " ('repulsión', 1),\n",
       " ('avezar', 1),\n",
       " ('habituarse', 1),\n",
       " ('avidez', 1),\n",
       " ('codicia', 1),\n",
       " ('axioma', 1),\n",
       " ('principio', 1),\n",
       " ('azotar', 1),\n",
       " ('zurrar', 1),\n",
       " ('azuzar', 1),\n",
       " ('incitar', 1),\n",
       " ('babilónico', 1),\n",
       " ('ostentoso', 1),\n",
       " ('baladí', 1),\n",
       " ('nimio', 1),\n",
       " ('baldar', 1),\n",
       " ('tullir', 1),\n",
       " ('baluarte', 1),\n",
       " ('fortaleza', 1),\n",
       " ('barahúnda', 1),\n",
       " ('baraúnda', 1),\n",
       " ('barbarismo', 1),\n",
       " ('barbarie', 1),\n",
       " ('barbitúrico', 1),\n",
       " ('sedante', 1),\n",
       " ('barragana', 1),\n",
       " ('bagasa', 1),\n",
       " ('bártulos', 1),\n",
       " ('enseres', 1),\n",
       " ('bastardo', 1),\n",
       " ('espurio', 1),\n",
       " ('bausano', 1),\n",
       " ('bobo', 1),\n",
       " ('befa', 1),\n",
       " ('mofa', 1),\n",
       " ('belicoso', 1),\n",
       " ('pendenciero', 1),\n",
       " ('bendecir', 1),\n",
       " ('ensalzar', 1),\n",
       " ('beneficiar', 1),\n",
       " ('favorecer', 1),\n",
       " ('benemérito', 1),\n",
       " ('estimable', 1),\n",
       " ('benévolo', 1),\n",
       " ('clemente', 1),\n",
       " ('benignidad', 1),\n",
       " ('bondad', 1),\n",
       " ('bicoca', 1),\n",
       " ('nadería', 1),\n",
       " ('bifurcar', 1),\n",
       " ('dividir', 1),\n",
       " ('bilioso', 1),\n",
       " ('malhumorado', 1),\n",
       " ('blasón', 1),\n",
       " ('honor', 1),\n",
       " ('bohemio', 1),\n",
       " ('gitano', 1),\n",
       " ('boleta', 1),\n",
       " ('comprobante', 1),\n",
       " ('bosquejar', 1),\n",
       " ('proyectar', 1),\n",
       " ('bravío', 1),\n",
       " ('indómito', 1),\n",
       " ('brevedad', 1),\n",
       " ('concisión', 1),\n",
       " ('brillante', 1),\n",
       " ('reluciente', 1),\n",
       " ('brío', 1),\n",
       " ('ánimo', 1),\n",
       " ('bruma', 1),\n",
       " ('niebla', 1),\n",
       " ('bucear', 1),\n",
       " ('sumergirse', 1),\n",
       " ('bullir', 1),\n",
       " ('agitarse', 1),\n",
       " ('burbujear', 1),\n",
       " ('hervir', 1),\n",
       " ('burdo', 1),\n",
       " ('tosco', 1),\n",
       " ('burlar', 1),\n",
       " ('chancear', 1),\n",
       " ('butaca', 1),\n",
       " ('asiento', 1),\n",
       " ('cabal', 1),\n",
       " ('Integro', 1),\n",
       " ('cábala', 1),\n",
       " ('cálculo', 1),\n",
       " ('cabildo', 1),\n",
       " ('junta', 1),\n",
       " ('cadalso', 1),\n",
       " ('patíbulo', 1),\n",
       " ('caduco', 1),\n",
       " ('viejo', 1),\n",
       " ('calamidad', 1),\n",
       " ('desgracia', 1),\n",
       " ('calaña', 1),\n",
       " ('ralea', 1),\n",
       " ('calentar', 1),\n",
       " ('caldear', 1),\n",
       " ('calibrar', 1),\n",
       " ('medir', 1),\n",
       " ('callar', 1),\n",
       " ('enmudecer', 1),\n",
       " ('calumniar', 1),\n",
       " ('infamar', 1),\n",
       " ('calvario', 1),\n",
       " ('cruz', 1),\n",
       " ('camal', 1),\n",
       " ('sensual', 1),\n",
       " ('camuflar', 1),\n",
       " ('encubrir', 1),\n",
       " ('candente', 1),\n",
       " ('ardiente', 1),\n",
       " ('candor', 1),\n",
       " ('inocencia', 1),\n",
       " ('canijo', 1),\n",
       " ('flaco', 1),\n",
       " ('canoro', 1),\n",
       " ('melodioso', 1),\n",
       " ('caótico', 1),\n",
       " ('confuso', 1),\n",
       " ('capear', 1),\n",
       " ('sortear', 1),\n",
       " ('capitalizar', 1),\n",
       " ('invertir', 1),\n",
       " ('caporal', 1),\n",
       " ('mayordomo', 1),\n",
       " ('captar', 1),\n",
       " ('lograr', 1),\n",
       " ('carácter', 1),\n",
       " ('condición', 1),\n",
       " ('carátula', 1),\n",
       " ('portada', 1),\n",
       " ('cardumen', 1),\n",
       " ('abundancia', 1),\n",
       " ('carecer', 1),\n",
       " ('faltar', 1),\n",
       " ('carestía', 1),\n",
       " ('escasez', 1),\n",
       " ('careta', 1),\n",
       " ('máscara', 1),\n",
       " ('cargo', 1),\n",
       " ('falta', 1),\n",
       " ('caricatura', 1),\n",
       " ('exageración', 1),\n",
       " ('caricia', 1),\n",
       " ('cariño', 1),\n",
       " ('cartapacio', 1),\n",
       " ('carpeta', 1),\n",
       " ('caso', 1),\n",
       " ('suceso', 1),\n",
       " ('castidad', 1),\n",
       " ('pureza', 1),\n",
       " ('castigar', 1),\n",
       " ('sancionar', 1),\n",
       " ('casual', 1),\n",
       " ('fortuito', 1),\n",
       " ('catalogar', 1),\n",
       " ('clasificar', 1),\n",
       " ('categoría', 1),\n",
       " ('clase', 1),\n",
       " ('categórico', 1),\n",
       " ('preciso', 1),\n",
       " ('catilinaria', 1),\n",
       " ('increpación', 1),\n",
       " ('caudal', 1),\n",
       " ('corriente', 1),\n",
       " ('cáustico', 1),\n",
       " ('mordaz', 1),\n",
       " ('cauteloso', 1),\n",
       " ('precavido', 1),\n",
       " ('cautivar', 1),\n",
       " ('fascinar', 1),\n",
       " ('cavilar', 1),\n",
       " ('pensar', 1),\n",
       " ('cazar', 1),\n",
       " ('perseguir', 1),\n",
       " ('cebar', 1),\n",
       " ('engordar', 1),\n",
       " ('cegar', 1),\n",
       " ('alucinar', 1),\n",
       " ('cejar', 1),\n",
       " ('ceder', 1),\n",
       " ('célebre', 1),\n",
       " ('famoso', 1),\n",
       " ('censo', 1),\n",
       " ('empadronamiento', 1),\n",
       " ('censurar', 1),\n",
       " ('criticar', 1),\n",
       " ('cercenar', 1),\n",
       " ('desunir', 1),\n",
       " ('cerebro', 1),\n",
       " ('talento', 1),\n",
       " ('ceremonia', 1),\n",
       " ('rito', 1),\n",
       " ('certero', 1),\n",
       " ('seguro', 1),\n",
       " ('cesar', 1),\n",
       " ('finalizar', 1),\n",
       " ('chabacano', 1),\n",
       " ('ordinario', 1),\n",
       " ('chacota', 1),\n",
       " ('broma', 1),\n",
       " ('chapucero', 1),\n",
       " ('chafallón', 1),\n",
       " ('charlatán', 1),\n",
       " ('hablador', 1),\n",
       " ('chasco', 1),\n",
       " ('burla', 1),\n",
       " ('chiflado', 1),\n",
       " ('alelado', 1),\n",
       " ('chisme', 1),\n",
       " ('murmuración', 1),\n",
       " ('chiste', 1),\n",
       " ('agudeza', 1),\n",
       " ('chocar', 1),\n",
       " ('tropezar', 1),\n",
       " ('chuchería', 1),\n",
       " ('fruslería', 1),\n",
       " ('chupar', 1),\n",
       " ('sorber', 1),\n",
       " ('churro', 1),\n",
       " ('buñuelo', 1),\n",
       " ('chusco', 1),\n",
       " ('chistoso', 1),\n",
       " ('chuzón', 1),\n",
       " ('ladino', 1),\n",
       " ('cicatero', 1),\n",
       " ('avaro', 1),\n",
       " ('cicerone', 1),\n",
       " ('gula', 1),\n",
       " ('ciego', 1),\n",
       " ('invidente', 1),\n",
       " ('ciencia', 1),\n",
       " ('sapiencia', 1),\n",
       " ('cierto', 1),\n",
       " ('manifiesto', 1),\n",
       " ('cilicio', 1),\n",
       " ('suplicio', 1),\n",
       " ('cínico', 1),\n",
       " ('desvergonzado', 1),\n",
       " ('circunloquio', 1),\n",
       " ('rodeo', 1),\n",
       " ('circunspección', 1),\n",
       " ('cautela', 1),\n",
       " ('circunvalar', 1),\n",
       " ('circular', 1),\n",
       " ('cisma', 1),\n",
       " ('rompimiento', 1),\n",
       " ('citar', 1),\n",
       " ('mencionar', 1),\n",
       " ('cizaña', 1),\n",
       " ('discordia', 1),\n",
       " ('clamar', 1),\n",
       " ('quejarse', 1),\n",
       " ('clandestino', 1),\n",
       " ('secreto', 1),\n",
       " ('clarividencia', 1),\n",
       " ('intuición', 1),\n",
       " ('clarividente', 1),\n",
       " ('perspicaz', 1),\n",
       " ('claudicar', 1),\n",
       " ('transigir', 1),\n",
       " ('clemencia', 1),\n",
       " ('piedad', 1),\n",
       " ('cloaca', 1),\n",
       " ('sumidero', 1),\n",
       " ('coacción', 1),\n",
       " ('imposición', 1),\n",
       " ('coadyuvar', 1),\n",
       " ('cooperar', 1),\n",
       " ('coartada', 1),\n",
       " ('pretexto', 1),\n",
       " ('cobardía', 1),\n",
       " ('timidez', 1),\n",
       " ('cobijar', 1),\n",
       " ('albergar', 1),\n",
       " ('cofradía', 1),\n",
       " ('hermandad', 1),\n",
       " ('cognoscible', 1),\n",
       " ('comprensible', 1),\n",
       " ('cohechar', 1),\n",
       " ('corromper', 1),\n",
       " ('cohesión', 1),\n",
       " ('coherencia', 1),\n",
       " ('cohibir', 1),\n",
       " ('contener', 1),\n",
       " ('cohorte', 1),\n",
       " ('séquito', 1),\n",
       " ('coincidir', 1),\n",
       " ('concordar', 1),\n",
       " ('colaborar', 1),\n",
       " ('auxiliar', 1),\n",
       " ('colapso', 1),\n",
       " ('desmoronamiento', 1),\n",
       " ('colateral', 1),\n",
       " ('adyacente', 1),\n",
       " ('colectivo', 1),\n",
       " ('común', 1),\n",
       " ('colegir', 1),\n",
       " ('deducir', 1),\n",
       " ('cólera', 1),\n",
       " ('rabia', 1),\n",
       " ('colgar', 1),\n",
       " ('pender', 1),\n",
       " ('colosal', 1),\n",
       " ('gigantesco', 1),\n",
       " ('colusión', 1),\n",
       " ('complicidad', 1),\n",
       " ('combatir', 1),\n",
       " ('luchar', 1),\n",
       " ('combustión', 1),\n",
       " ('ignición', 1),\n",
       " ('comentar', 1),\n",
       " ('desarrollar', 1),\n",
       " ('comerciar', 1),\n",
       " ('negociar', 1),\n",
       " ('cometer', 1),\n",
       " ('incidir', 1),\n",
       " ('compacto', 1),\n",
       " ('macizo', 1),\n",
       " ('compadecer', 1),\n",
       " ('apenar', 1),\n",
       " ('compaginar', 1),\n",
       " ('armonizar', 1),\n",
       " ('comparecer', 1),\n",
       " ('acudir', 1),\n",
       " ('compeler', 1),\n",
       " ('forzar', 1),\n",
       " ('compendio', 1),\n",
       " ('resumen', 1),\n",
       " ('compenetrarse', 1),\n",
       " ('entenderse', 1),\n",
       " ('compensar', 1),\n",
       " ('resarcir', 1),\n",
       " ('competencia', 1),\n",
       " ('jurisdicción', 1),\n",
       " ('competir', 1),\n",
       " ('contender', 1),\n",
       " ('compilar', 1),\n",
       " ('seleccionar', 1),\n",
       " ('complacer', 1),\n",
       " ('agradar', 1),\n",
       " ('completar', 1),\n",
       " ('concluir', 1),\n",
       " ('complicar', 1),\n",
       " ('enredar', 1),\n",
       " ('cómplice', 1),\n",
       " ('partícipe', 1),\n",
       " ('componenda', 1),\n",
       " ('arreglo', 1),\n",
       " ('compostura', 1),\n",
       " ('reparación', 1),\n",
       " ('comprender', 1),\n",
       " ('entender', 1),\n",
       " ('comprensión', 1),\n",
       " ('penetración', 1),\n",
       " ('comprimir', 1),\n",
       " ('apretar', 1),\n",
       " ('compromiso', 1),\n",
       " ('promesa', 1),\n",
       " ('compulsar', 1),\n",
       " ('confrontar', 1),\n",
       " ('compunción', 1),\n",
       " ('contrición', 1),\n",
       " ('computar', 1),\n",
       " ('contar', 1),\n",
       " ('comunicación', 1),\n",
       " ('escrito', 1),\n",
       " ('comunidad', 1),\n",
       " ('corporación', 1),\n",
       " ('conato', 1),\n",
       " ('indicio', 1),\n",
       " ('concatenar', 1),\n",
       " ('encadenar', 1),\n",
       " ('conceder', 1),\n",
       " ('otorgar', 1),\n",
       " ('concepción', 1),\n",
       " ('proyecto', 1),\n",
       " ('concepto', 1),\n",
       " ('idea', 1),\n",
       " ('conciencia', 1),\n",
       " ('razón', 1),\n",
       " ('concienzudo', 1),\n",
       " ('responsable', 1),\n",
       " ('conciliación', 1),\n",
       " ('avenencia', 1),\n",
       " ('conciso', 1),\n",
       " ('breve', 1),\n",
       " ('cónclave', 1),\n",
       " ('reunido', 1),\n",
       " ('conclusión', 1),\n",
       " ('final', 1),\n",
       " ('concluyente', 1),\n",
       " ('decisivo', 1),\n",
       " ('concomitante', 1),\n",
       " ('concurrente', 1),\n",
       " ('concordia', 1),\n",
       " ('armonía', 1),\n",
       " ('concreción', 1),\n",
       " ('acumulación', 1),\n",
       " ('concupiscencia', 1),\n",
       " ('sexualidad', 1),\n",
       " ('concusión', 1),\n",
       " ('conmoción', 1),\n",
       " ('condonar', 1),\n",
       " ('perdonar', 1),\n",
       " ('conducente', 1),\n",
       " ('propio', 1),\n",
       " ('conducto', 1),\n",
       " ('canal', 1),\n",
       " ('confabular', 1),\n",
       " ('conspirar', 1),\n",
       " ('confeccionar', 1),\n",
       " ('preparar', 1),\n",
       " ('conferir', 1),\n",
       " ('acordar', 1),\n",
       " ('confesión', 1),\n",
       " ('declaración', 1),\n",
       " ('confianza', 1),\n",
       " ('seguridad', 1),\n",
       " ('confiar', 1),\n",
       " ('fiar', 1),\n",
       " ('configurar', 1),\n",
       " ('conformar', 1),\n",
       " ('confirmar', 1),\n",
       " ('corroborar', 1),\n",
       " ('confiscar', 1),\n",
       " ('decomisar', 1),\n",
       " ('conflictivo', 1),\n",
       " ('intolerante', 1),\n",
       " ('confluir', 1),\n",
       " ('converger', 1),\n",
       " ('conforme', 1),\n",
       " ('acorde', 1),\n",
       " ('confortar', 1),\n",
       " ('animar', 1),\n",
       " ('confusión', 1),\n",
       " ('mezcolanza', 1),\n",
       " ('congelar', 1),\n",
       " ('helar', 1),\n",
       " ('congénito', 1),\n",
       " ('hereditario', 1),\n",
       " ('congestionar', 1),\n",
       " ('aglomerar', 1),\n",
       " ('conglomerado', 1),\n",
       " ('aglomerado', 1),\n",
       " ('congregar', 1),\n",
       " ('juntar', 1),\n",
       " ('congruencia', 1),\n",
       " ('oportunidad', 1),\n",
       " ('conjeturar', 1),\n",
       " ('suponer', 1),\n",
       " ('conjurar', 1),\n",
       " ('evitar', 1),\n",
       " ('connotado', 1),\n",
       " ('relacionado', 1),\n",
       " ('conocer', 1),\n",
       " ('saber', 1),\n",
       " ('consabido', 1),\n",
       " ('aludido', 1),\n",
       " ('consanguinidad', 1),\n",
       " ('afinidad', 1),\n",
       " ('consecución', 1),\n",
       " ('obtención', 1),\n",
       " ('consecuente', 1),\n",
       " ('razonable', 1),\n",
       " ('consejo', 1),\n",
       " ('reunión', 1),\n",
       " ('consentir', 1),\n",
       " ('acceder', 1),\n",
       " ('consigna', 1),\n",
       " ('orden', 1),\n",
       " ('consolidar', 1),\n",
       " ('asegurar', 1),\n",
       " ('consorcio', 1),\n",
       " ('compañía', 1),\n",
       " ('constancia', 1),\n",
       " ('tenacidad', 1),\n",
       " ('constituir', 1),\n",
       " ('formar', 1),\n",
       " ('constricción', 1),\n",
       " ('encogimiento', 1),\n",
       " ('consuelo', 1),\n",
       " ('alivio', 1),\n",
       " ('consumir', 1),\n",
       " ('gestar', 1),\n",
       " ('consunción', 1),\n",
       " ('agotamiento', 1),\n",
       " ('contactar', 1),\n",
       " ('conectar', 1),\n",
       " ('contienda', 1),\n",
       " ('disputa', 1),\n",
       " ('contigüidad', 1),\n",
       " ('cercanía', 1),\n",
       " ('continencia', 1),\n",
       " ('moderación', 1),\n",
       " ('contingencia', 1),\n",
       " ('eventualidad', 1),\n",
       " ('continuo', 1),\n",
       " ('incesante', 1),\n",
       " ('contorsión', 1),\n",
       " ('contracción', 1),\n",
       " ('contraer', 1),\n",
       " ('adquirir', 1),\n",
       " ('contraponer', 1),\n",
       " ('oponer', 1),\n",
       " ('contrariar', 1),\n",
       " ('dificultar', 1),\n",
       " ('contrarrestar', 1),\n",
       " ('afrontar', 1),\n",
       " ('contraste', 1),\n",
       " ('oposición', 1),\n",
       " ('contrincante', 1),\n",
       " ('émulo', 1),\n",
       " ('contrito', 1),\n",
       " ('arrepentido', 1),\n",
       " ('control', 1),\n",
       " ('vigilancia', 1),\n",
       " ('controversia', 1),\n",
       " ('polémica', 1),\n",
       " ('contusión', 1),\n",
       " ('lesión', 1),\n",
       " ('convalidar', 1),\n",
       " ('revalidar', 1),\n",
       " ('convencer', 1),\n",
       " ('persuadir', 1),\n",
       " ('convergencia', 1),\n",
       " ('confluencia', 1),\n",
       " ('copiar', 1),\n",
       " ('reproducir', 1),\n",
       " ('coraza', 1),\n",
       " ('armadura', 1),\n",
       " ('cordial', 1),\n",
       " ('afable', 1),\n",
       " ('corporal', 1),\n",
       " ('somático', 1),\n",
       " ('corpulento', 1),\n",
       " ('corpudo', 1),\n",
       " ('corrección', 1),\n",
       " ('enmienda', 1),\n",
       " ('correlación', 1),\n",
       " ('analogía', 1),\n",
       " ('corretaje', 1),\n",
       " ('correduría', 1),\n",
       " ('corrido', 1),\n",
       " ('ducho', 1),\n",
       " ('corroboración', 1),\n",
       " ('aseveración', 1),\n",
       " ('corroer', 1),\n",
       " ('desgastar', 1),\n",
       " ('cortapisa', 1),\n",
       " ('obstáculo', 1),\n",
       " ('costumbre', 1),\n",
       " ('hábito', 1),\n",
       " ('coterráneo', 1),\n",
       " ('paisano', 1),\n",
       " ('coyuntura', 1),\n",
       " ('unión', 1),\n",
       " ('crédulo', 1),\n",
       " ('cándido', 1),\n",
       " ('crepúsculo', 1),\n",
       " ('penumbra', 1),\n",
       " ('criar', 1),\n",
       " ('crear', 1),\n",
       " ('crisis', 1),\n",
       " ('desequilibrio', 1),\n",
       " ('crítico', 1),\n",
       " ('censor', 1),\n",
       " ('crudeza', 1),\n",
       " ('severidad', 1),\n",
       " ('cuantía', 1),\n",
       " ('cantidad', 1),\n",
       " ('cuartel', 1),\n",
       " ('acantonamiento', 1),\n",
       " ('cuestión', 1),\n",
       " ('materia', 1),\n",
       " ('culminación', 1),\n",
       " ('cúspide', 1),\n",
       " ('cultivar', 1),\n",
       " ('laborar', 1),\n",
       " ('cupido', 1),\n",
       " ('amor', 1),\n",
       " ('cupo', 1),\n",
       " ('porción', 1),\n",
       " ('cúpula', 1),\n",
       " ('bóveda', 1),\n",
       " ('curia', 1),\n",
       " ('cancillería', 1),\n",
       " ('curiosidad', 1),\n",
       " ('indiscreción', 1),\n",
       " ('cursar', 1),\n",
       " ('seguir', 1),\n",
       " ('cursi', 1),\n",
       " ('afectado', 1),\n",
       " ('custodiar', 1),\n",
       " ('proteger', 1),\n",
       " ('dádiva', 1),\n",
       " ('obsequio', 1),\n",
       " ('dama', 1),\n",
       " ('señora', 1),\n",
       " ('damnificar', 1),\n",
       " ('perjudicar', 1),\n",
       " ('datar', 1),\n",
       " ('fechar', 1),\n",
       " ('debatir', 1),\n",
       " ('disputar', 1),\n",
       " ('débil', 1),\n",
       " ('endeble', 1),\n",
       " ('debutar', 1),\n",
       " ('estrenar', 1),\n",
       " ('decaer', 1),\n",
       " ('debilitar', 1),\n",
       " ('deceso', 1),\n",
       " ('muerte', 1),\n",
       " ('dechado', 1),\n",
       " ('ejemplo', 1),\n",
       " ('declamar', 1),\n",
       " ('recitar', 1),\n",
       " ('declarar', 1),\n",
       " ('manifestar', 1),\n",
       " ('declinación', 1),\n",
       " ('declive', 1),\n",
       " ('decorar', 1),\n",
       " ('ornar', 1),\n",
       " ('decretar', 1),\n",
       " ('decidir', 1),\n",
       " ('deducción', 1),\n",
       " ('consecuencia', 1),\n",
       " ('defecto', 1),\n",
       " ('carencia', 1),\n",
       " ('deflagar', 1),\n",
       " ('arder', 1),\n",
       " ('deformar', 1),\n",
       " ('falsear', 1),\n",
       " ('defraudar', 1),\n",
       " ('engañar', 1),\n",
       " ('degeneración', 1),\n",
       " ('decadencia', 1),\n",
       " ('degradar', 1),\n",
       " ('rebajar', 1),\n",
       " ('degustar', 1),\n",
       " ('probar', 1),\n",
       " ('deidificar', 1),\n",
       " ('divinizar', 1),\n",
       " ('dejación', 1),\n",
       " ('abandono', 1),\n",
       " ...]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_counter = Counter(words)\n",
    "print(\"Unique words:\", len(word_counter.most_common()))\n",
    "word_counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:  4500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[3, 4],\n",
       " [4, 3],\n",
       " [5, 6],\n",
       " [6, 5],\n",
       " [7, 8],\n",
       " [8, 7],\n",
       " [9, 10],\n",
       " [10, 9],\n",
       " [11, 12],\n",
       " [12, 11]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# Let's now create a word dictionary\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(pairs)\n",
    "\n",
    "tokenized_pairs = tokenizer.texts_to_sequences(pairs)\n",
    "\n",
    "print(\"Number of words: \", len(tokenizer.word_index))\n",
    "tokenized_pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['patrocinar', 'abominar', 'acrecentar', 'abrasar']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will help us visualize the outputs much better\n",
    "def indexToWord(indices):\n",
    "    words = []\n",
    "    for idx in indices:\n",
    "        words.append(tokenizer.index_word[idx])\n",
    "    return words\n",
    "\n",
    "indexToWord([12, 15, 34, 21])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in: One Hot - out: One Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4502, 4501) (4502, 4501)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['abalanzar', 'equilibrar']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "one_hot_pairs = to_categorical(tokenized_pairs)\n",
    "\n",
    "one_hot_x = one_hot_pairs[:, 0] \n",
    "one_hot_y = one_hot_pairs[:, 1]\n",
    "\n",
    "print(one_hot_x.shape, one_hot_y.shape)\n",
    "indexToWord([np.argmax(one_hot_x[0]), np.argmax(one_hot_y[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_80 (Dense)             (None, 300)               1350600   \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 4501)              1354801   \n",
      "=================================================================\n",
      "Total params: 2,705,401\n",
      "Trainable params: 2,705,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "4502/4502 [==============================] - 5s 1ms/step - loss: 8.4186 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "4502/4502 [==============================] - 3s 730us/step - loss: 8.3844 - acc: 0.4189\n",
      "Epoch 3/5\n",
      "4502/4502 [==============================] - 3s 729us/step - loss: 8.3266 - acc: 0.6610\n",
      "Epoch 4/5\n",
      "4502/4502 [==============================] - 3s 734us/step - loss: 8.2380 - acc: 0.6182\n",
      "Epoch 5/5\n",
      "4502/4502 [==============================] - 3s 736us/step - loss: 8.1255 - acc: 0.6499\n",
      "Expected: ['equilibrar', 'infortunio', 'consciente', 'apariencia']\n",
      "Predicted: ['equilibrar', 'infortunio', 'consciente', 'apariencia']\n"
     ]
    }
   ],
   "source": [
    "#Let's now do a very simple model that will try to learn using One Hot input and output\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(300, activation=\"relu\", input_shape=(4501,)))\n",
    "model.add(Dense(4501, activation=\"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(one_hot_x, one_hot_y, shuffle=True, epochs=5, batch_size=256)\n",
    "\n",
    "# Test trained model\n",
    "tokenized_y = np.array(tokenized_pairs)[:, 1]\n",
    "expected = indexToWord([tokenized_y[0], tokenized_y[100], tokenized_y[200], tokenized_y[300]])\n",
    "\n",
    "pred = model.predict(np.array([one_hot_x[0], one_hot_x[100], one_hot_x[200], one_hot_x[300]]))\n",
    "pred_tokenized = [np.argmax(p) for p in pred]\n",
    "pred_words = indexToWord(pred_tokenized)\n",
    "\n",
    "print(\"Expected:\", expected)\n",
    "print(\"Predicted:\", pred_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that by using one hot encoddings the model can learn but after 5 epochs it only achieves 65% accuracy, this it's to be expected as a vector of size 4501 with all zeros except for a single 1 has a VERY low activation power and it's very hard for it to learn... :(  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in: Raw integer - out: One Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_66 (Dense)             (None, 300)               600       \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 4501)              1354801   \n",
      "=================================================================\n",
      "Total params: 1,355,401\n",
      "Trainable params: 1,355,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "4502/4502 [==============================] - 4s 856us/step - loss: 15.9628 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "4502/4502 [==============================] - 3s 737us/step - loss: 15.8434 - acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "4502/4502 [==============================] - 3s 655us/step - loss: 15.8257 - acc: 0.0000e+00\n",
      "Epoch 4/5\n",
      "4502/4502 [==============================] - 3s 608us/step - loss: 15.8171 - acc: 0.0000e+00\n",
      "Epoch 5/5\n",
      "4502/4502 [==============================] - 3s 754us/step - loss: 15.8140 - acc: 0.0000e+00\n",
      "Expected: ['equilibrar', 'infortunio', 'consciente', 'apariencia']\n",
      "Predicted: ['asomar', 'asomar', 'asomar', 'asomar']\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = Sequential()\n",
    "model.add(Dense(300, activation=\"relu\", input_shape=(1,))) # same as before, but it accepts a single value as input: the raw token\n",
    "model.add(Dense(4501, activation=\"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "tokenized_x = np.array(tokenized_pairs)[:, 0]\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(tokenized_x, one_hot_y, shuffle=True, epochs=5, batch_size=256)\n",
    "\n",
    "## Test trained model\n",
    "pred = model.predict(np.array([tokenized_x[0], tokenized_x[100], tokenized_x[200], tokenized_x[300]]))\n",
    "pred_tokenized = [np.argmax(p) for p in pred]\n",
    "pred_words = indexToWord(pred_tokenized)\n",
    "\n",
    "print(\"Expected:\", expected)\n",
    "print(\"Predicted:\", pred_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the input is a raw integer the model doesn't seem to learn anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in: Embeddings - out: One Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 1, 100)            450100    \n",
      "_________________________________________________________________\n",
      "reshape_12 (Reshape)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 300)               30300     \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 4501)              1354801   \n",
      "=================================================================\n",
      "Total params: 1,835,201\n",
      "Trainable params: 1,835,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "4502/4502 [==============================] - 5s 1ms/step - loss: 8.4182 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "4502/4502 [==============================] - 3s 760us/step - loss: 8.3892 - acc: 0.2614\n",
      "Epoch 3/5\n",
      "4502/4502 [==============================] - 3s 648us/step - loss: 8.3343 - acc: 0.3396\n",
      "Epoch 4/5\n",
      "4502/4502 [==============================] - 3s 626us/step - loss: 8.2231 - acc: 0.3563\n",
      "Epoch 5/5\n",
      "4502/4502 [==============================] - 3s 685us/step - loss: 8.0313 - acc: 0.4200\n",
      "Expected: ['equilibrar', 'infortunio', 'consciente', 'apariencia']\n",
      "Predicted: ['equilibrar', 'impericia', 'consciente', 'impericia']\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, Reshape\n",
    "# Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(4501, 100, input_length=1)) # the input shape is the same, but we transform it to an embedding before passing it further\n",
    "model.add(Reshape((100,)))\n",
    "model.add(Dense(300, activation=\"relu\"))\n",
    "model.add(Dense(4501, activation=\"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "tokenized_x = np.array(tokenized_pairs)[:, 0]\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(tokenized_x, one_hot_y, shuffle=True, epochs=5, batch_size=256)\n",
    "\n",
    "## Test trained model\n",
    "pred = model.predict(np.array([tokenized_x[0], tokenized_x[100], tokenized_x[200], tokenized_x[300]]))\n",
    "pred_tokenized = [np.argmax(p) for p in pred]\n",
    "pred_words = indexToWord(pred_tokenized)\n",
    "\n",
    "print(\"Expected:\", expected)\n",
    "print(\"Predicted:\", pred_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so good :( only 42% accuracy\n",
    "Maybe the big bottleneck here is the output... let's start fiddling with that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in: One Hot - out: Raw integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_84 (Dense)             (None, 300)               1350600   \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 1,350,901\n",
      "Trainable params: 1,350,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "4502/4502 [==============================] - 2s 508us/step - loss: 6748634.2079 - acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "4502/4502 [==============================] - 1s 211us/step - loss: 6746899.4187 - acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "4502/4502 [==============================] - 1s 165us/step - loss: 6744029.1790 - acc: 4.4425e-04\n",
      "Epoch 4/5\n",
      "4502/4502 [==============================] - 1s 155us/step - loss: 6739843.4809 - acc: 4.4425e-04\n",
      "Epoch 5/5\n",
      "4502/4502 [==============================] - 1s 171us/step - loss: 6734239.3632 - acc: 4.4425e-04\n",
      "[4, 4, 4, 4]\n",
      "Expected: ['equilibrar', 'infortunio', 'consciente', 'apariencia']\n",
      "Predicted: ['equilibrar', 'equilibrar', 'equilibrar', 'equilibrar']\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, activation=\"relu\", input_shape=(4501,)))\n",
    "model.add(Dense(1)) # we need the outputs to be values so we'll use a relu activation\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(one_hot_x, tokenized_y, epochs=5, batch_size=256)\n",
    "\n",
    "# Test trained model\n",
    "expected = indexToWord([tokenized_y[0], tokenized_y[100], tokenized_y[200], tokenized_y[300]])\n",
    "\n",
    "pred = model.predict(np.array([one_hot_x[0], one_hot_x[100], one_hot_x[200], one_hot_x[300]]))\n",
    "pred_tokenized = [x[0] for x in np.rint(pred).astype(np.int16)]\n",
    "print(pred_tokenized)\n",
    "pred_words = indexToWord(pred_tokenized)\n",
    "\n",
    "print(\"Expected:\", expected)\n",
    "print(\"Predicted:\", pred_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a raw integer as output also doesn't work :( the model has a very high loss that seems to be reducing but very VERY slowly, also, the model risks of outputing the wrong word just because thge value was 34.4 instead of 34.6 when the expected value is 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in: One Hot - out: Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_88 (Dense)             (None, 300)               1350600   \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 4501)              1354801   \n",
      "=================================================================\n",
      "Total params: 2,705,401\n",
      "Trainable params: 2,705,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "4502/4502 [==============================] - 3s 729us/step - loss: 8.4185 - acc: 2.2212e-04\n",
      "Epoch 2/5\n",
      "4502/4502 [==============================] - 2s 456us/step - loss: 8.3842 - acc: 0.4060\n",
      "Epoch 3/5\n",
      "4502/4502 [==============================] - 2s 455us/step - loss: 8.3263 - acc: 0.5538\n",
      "Epoch 4/5\n",
      "4502/4502 [==============================] - 2s 462us/step - loss: 8.2375 - acc: 0.2750\n",
      "Epoch 5/5\n",
      "4502/4502 [==============================] - 2s 454us/step - loss: 8.1253 - acc: 0.5860\n",
      "Expected: ['equilibrar', 'infortunio', 'consciente', 'apariencia']\n",
      "Predicted: ['equilibrar', 'infortunio', 'consciente', 'apariencia']\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(300, activation=\"relu\", input_shape=(4501,)))\n",
    "model.add(Dense(4501, activation=\"softmax\")) # the model will output a one-hot encoded value, but thanks to the loss function it will be associated with an embedding\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(one_hot_x, tokenized_y, shuffle=True, epochs=5, batch_size=256)\n",
    "\n",
    "# Test trained model\n",
    "tokenized_y = np.array(tokenized_pairs)[:, 1]\n",
    "expected = indexToWord([tokenized_y[0], tokenized_y[100], tokenized_y[200], tokenized_y[300]])\n",
    "\n",
    "pred = model.predict(np.array([one_hot_x[0], one_hot_x[100], one_hot_x[200], one_hot_x[300]]))\n",
    "pred_tokenized = [np.argmax(p) for p in pred]\n",
    "pred_words = indexToWord(pred_tokenized)\n",
    "\n",
    "print(\"Expected:\", expected)\n",
    "print(\"Predicted:\", pred_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performs very similarly to the One Hot - One Hot model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in: Embeddings - out: Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 1, 200)            900200    \n",
      "_________________________________________________________________\n",
      "reshape_16 (Reshape)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 300)               60300     \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 4501)              1354801   \n",
      "=================================================================\n",
      "Total params: 2,315,301\n",
      "Trainable params: 2,315,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "4502/4502 [==============================] - 4s 799us/step - loss: 8.4191 - acc: 2.2212e-04\n",
      "Epoch 2/15\n",
      "4502/4502 [==============================] - 2s 436us/step - loss: 8.3781 - acc: 0.3892\n",
      "Epoch 3/15\n",
      "4502/4502 [==============================] - 2s 436us/step - loss: 8.3029 - acc: 0.6877\n",
      "Epoch 4/15\n",
      "4502/4502 [==============================] - 2s 443us/step - loss: 8.1534 - acc: 0.6559\n",
      "Epoch 5/15\n",
      "4502/4502 [==============================] - 2s 450us/step - loss: 7.8940 - acc: 0.6821\n",
      "Epoch 6/15\n",
      "4502/4502 [==============================] - 2s 418us/step - loss: 7.4806 - acc: 0.8878\n",
      "Epoch 7/15\n",
      "4502/4502 [==============================] - 2s 442us/step - loss: 6.8592 - acc: 0.9707\n",
      "Epoch 8/15\n",
      "4502/4502 [==============================] - 2s 425us/step - loss: 5.9812 - acc: 0.9971\n",
      "Epoch 9/15\n",
      "4502/4502 [==============================] - 2s 394us/step - loss: 4.8011 - acc: 0.9993\n",
      "Epoch 10/15\n",
      "4502/4502 [==============================] - 2s 392us/step - loss: 3.3037 - acc: 0.9991\n",
      "Epoch 11/15\n",
      "4502/4502 [==============================] - 2s 379us/step - loss: 1.6497 - acc: 0.9993\n",
      "Epoch 12/15\n",
      "4502/4502 [==============================] - 2s 395us/step - loss: 0.4976 - acc: 0.9993\n",
      "Epoch 13/15\n",
      "4502/4502 [==============================] - 2s 407us/step - loss: 0.1468 - acc: 0.9996\n",
      "Epoch 14/15\n",
      "4502/4502 [==============================] - 2s 416us/step - loss: 0.0712 - acc: 0.9996\n",
      "Epoch 15/15\n",
      "4502/4502 [==============================] - 2s 395us/step - loss: 0.0486 - acc: 0.9991\n",
      "Expected: ['equilibrar', 'infortunio', 'consciente', 'apariencia']\n",
      "Predicted: ['equilibrar', 'infortunio', 'consciente', 'apariencia']\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(4501, 200, input_length=1)) # the input shape is the same, but we transform it to an embedding before passing it further\n",
    "model.add(Reshape((200,)))\n",
    "model.add(Dense(300, activation=\"relu\"))\n",
    "model.add(Dense(4501, activation=\"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "tokenized_x = np.array(tokenized_pairs)[:, 0]\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(tokenized_x, tokenized_y, epochs=15, batch_size=256)\n",
    "\n",
    "## Test trained model\n",
    "pred = model.predict(np.array([tokenized_x[0], tokenized_x[100], tokenized_x[200], tokenized_x[300]]))\n",
    "pred_tokenized = [np.argmax(p) for p in pred]\n",
    "pred_words = indexToWord(pred_tokenized)\n",
    "\n",
    "print(\"Expected:\", expected)\n",
    "print(\"Predicted:\", pred_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is definitely the best model, as we achieved 82% accuracy after only 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precisión   ['exactitud']\n"
     ]
    }
   ],
   "source": [
    "word = 'precisión'\n",
    "word_index = tokenizer.word_index[word]\n",
    "synonim = model.predict(np.array([word_index]))[0]\n",
    "synonim = indexToWord([np.argmax(synonim)])\n",
    "\n",
    "print(word, \" \", synonim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
